# Text Processing (2)

Source: https://wikidocs.net/21694, https://wikidocs.net/21698, https://wikidocs.net/21693, https://wikidocs.net/21707, https://wikidocs.net/22530, https://wikidocs.net/21703, https://wikidocs.net/31766, https://wikidocs.net/22647, https://wikidocs.net/22592, https://wikidocs.net/33274
NLP에 있어서 Text Processing은 매우 중요한 작업이다.

---

### 2. 정제(Cleaning) & 정규화(Normalization)

코퍼스에서 용도에 맞게 토큰을 분류하는 작업을 토큰화라고 하며, 토큰화 작업 전/후에는 텍스트 데이터를 용도에 맞게 정제 및 정규화하는 일이 항상 함께한다.
정제 및 정규화의 목적은 각각 다음과 같다.

- 정제(cleaning): 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.
- 정규화(Nomoralization): 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어 준다.

정제 작업은 토큰화 작업에 방해가 되는 부분들을 배제시키고, 토큰화 작업을 수행하기 위해서 토큰화 작업보다 앞서 이루어지기도 하지만, 토큰화 작업 이우헤도 여전히 남아있는 노이즈들을 제거하기 위해 지속적으로 이루어지고 한다.
사실 완벽한 정제 작업은 어려운 편에 속하기에, 대부분의 경우 '이 정도면 됐다.'라는 일종의 합의점을 찾기도 한다.

---

### 2.1 규칙에 기반한 표기가 다른 단어들의 통합

필요에 따라 직접 코딩을 통해 정의할 수 있는 정규화 규칙의 예로서 같은 의미르르 갖고있음에도, 표기가 다른 단어들을 하나의 단어로 정규화하는 방법을 사용할 수 있다.

가령, USA와 US는 같은 의미를 가지므로, 하나의 단어로 정규화할 수 있다.
uh-huh와 uhhuh는 형태는 다르지만 여전히 같은 의미를 갖고 있다.
이러한 정규화를 거치게 되면, US를 찾아도 USA도 함께 찾을 수 있을 것이다.

---

### 2.2 대/소문자 통합

영어권 안에서 대, 소문자를 통합하는 것은 단어의 개수를 줄일 수 있는 또 다른 정규화 방법이다.
영어권 언어에서 대문자는 문장의 맨 앞 등과 같은 특정 상황에서만 쓰이고, 대부분의 글은 소문자로 작성되기 때문에 대/소문자 통합 작업은 대부분 대문자를 소문자로 변환하는 소문자 변환작업으로 이루어지게 된다.

예시로, Automobile이라는 단어가 문장의 첫 단어이였기 때문에 A가 대문자이라고 가정해보자.
여기에 소문자 변환을 사용하면, automobile을 찾는 질의에서, 결과로서 Automobile도 찾을 수 있게 된다.

유사하게 검색 엔진에서 사용자가 페라리 차량에 관심이 있어서 페라리를 검색한다고 가정해보자.
엄밀히 말하면, 사실 사용자가 검색을 통해 찾고자하는 결과는 'a Ferrari car'라고 봐야한다.
하지만 검색 엔진은 소문자 변환을 적용했을 것이기에 ferrari만 검색해도 원하는 결과를 얻을 수 있을 것이다.

물론, 대문자와 소문자를 무작정 통합해서는 안된다.
대문자와 소문자가 구분되어야 하는 경우도 존재하기 때문이다.
가령 미국을 뜻하는 단어 US와 우리를 뜻하는 us는 구분되어야 한다.
또한 회사 이름이나, 사람 이름등은 대문자로 유지되어야 하는게 맞다.

모든 토큰을 소문자로 만든 것이 문제를 가져온다면, 또 다른 대안은 일부만 소문자로 변환시키는 방법도 있습니다.
그럼 문장의 맨 앞에서 나오는 단어의 대문자만 소문자로 바꾸고, 다른 단어들은 전부 대문자인 상태로 두는 것은 어떨까?

사실 이러한 작업은 더 많은 변수를 이용해서 소문자 변환을 언제 사용할지 결정하는 ML시퀀스 모델로 더 정확하게 진행시킬 수 있다.
하지만 만약 올바른 단어를 얻고 싶은 상황에서, 훈련에 사용하는 코퍼스가 사용자들이 단어의 대/소문자의 올바른 사용 방법과 상관없이 소문자를 사용하는 사람들로 부터 나온 데이터라면 이러한 방법 또한 그다지 도움이 되지 않을 수도 있다.
결국에는 예외 사항을 크게 고려하지 않고, 모든 코퍼스를 소문자로 바꾸는 것이 종종 더 실용적인 해결책이 되기도 한다.

---

### 2.3 불필요한 단어의 제거(Removing Unnecessary Words)

정제 작업에서 제거해야하는 노이즈 데이터는 자연가 아니면서, 아무 의미도 갖지 않는 글자들(특수문자 등)을 의미하기도 하지만, 분석하고자 하는 목적에 맞지 않는 불필요한 단어들을 노이즈 데이터이라고 하기도 한다.

불필요한 단어들을 제거하는 방법으로는 불용어 제거와 등장 빈도가 적은 단어, 길이가 짧은 단어들을 제거하는 방법이 있다.

1. 등장 빈도가 적은 단어(Removing Rare words)
   텍스트 데이터에서 너무 적게 등장햇 자연어 처리에 도움이 되지 않는 단어들이 존재한다.
   예시로, 입력된 메일이 정상 메일인지 스팸 메일인지 분류하는 분류기를 설계한다고 가정하자.
   총 100,000개의 메일을 가지고 정상 메일에서는 어떤 단어들이 주로 등장하고, 스팸 메일에서는 어떤 단어들이 주로 등장하는지를 가지고 설계하고자 한다.
   그런데 이때 100,000개의 메일 데이터에서 5번 밖에 등장하지 않는 단어가 있다면, 이 단어는 직관적으로 분류에 거의 도움이 되지 않을 것임을 알 수 있다.

2. 길이가 짧은 단어(Removing words with very a short length)
   영어권 언어에서 길이가 짧은 단어를 삭제하는 것만으로도 어느 정도 NLP에서 크게 의미 없는 단어들을 제거하는 효과를 볼 수 있다고 알려져 있다.
   즉, 영어권 언어에서 길이가 짧은 단어들은 대부분 불용어에 속한다.
   사실 길이가 짧은 단어를 제거하는 2차 이유는 길이를 조건으로 텍스트를 삭제하면서 간어가 아닌 구두점들 까지도 한 번에 제거하기 위함도 있다.
   하지만 한국어에서는 길이가 짧은 단어라고 삭제하는 방법이 크게 유효하지 않을 수 도 있는데, 그 이유를 정리해보자.

   단정지어 말하수는 없지만, 영어 단어의 평균 길이는 6~7자 정도이며, 한국어 단어의 평균 길이는 2~3자로  추정되고 있다.
   두 나라의 단어 평균 길이가 몇 인지에 대해서는 확실히 말하기 힘들지만, 그럼에도 확실한 사실은 영어 단어의 길이가 한국어 단어의 길이보다는 평균적으로 길다는 점이다.

   이는 영어 단어와 한국어 단어에서 각 한 글자가 가진 의미의 크기가 다르다는 점에서 기인한다.
   한국어 단어에는 한자어가 많고, 한 글자로만으로도 의미를 가지는 경우도 많다.
   예를 들어 '학교'라는 한국어 단어를 생각해보면, 배울 학(學)과 학교 교(校)로 글자 하나, 하나가 이미 함축적인 의미를 갖고있어 두 글자로만으로 학교란 단어를 표현한다.
   하지만 영어의 경우에는 학교라는 글자를 표현하기 위해서 s, c, h, o, o, l이라는 총 6개의 글자가 필요하다.
   다른 예로, 전설 속 동물인 용(龍)을 표현하기 위해서는 한국어로는 한 글자면 충분하지만, 영어에서는 d, r, a, g, o, n이라는 총 6개의 글자가 필요하다.

   이러한 특성으로 인해 영어는 길이가 2~3자 이하인 단어를 제거하는 것만으로도 크게 읨를 갖지 못하는 단어를 줄이는 효과를 갖고 있다.
   예를 들어 갖고 있는 텍스트 데이터에서 길이가 1자인 단어를 제거하는 코드를 수행하면 대부분의 NLP에서 의미를 갖지 못하는 단어인 관사 'a'와 주어로 쓰이는 'i'가 제거된다.
   마찬가지로 길이가 2자인 단어를 제거한다고 하면 it, at, to, on, in, by 등과 같은 대부분 불용어에 해당되는 단어들이 제거된다.
   필요에 따라서, 길이가 3인 단어도 제거할 수 있지만, 이 경우 fox, dog, car 등 길이가 3자인 명사들이 제거 되기 시작되어 사용하고자 하는 데이터에서 해당 방법을 사용해도 되는지에 대한 고민이 필요하다. 

   ```python
   # 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제
   import re
   text = "I was wondering if anyone out there could enlighten me on this car."
   shortword = re.compile(r'\W*\b\w{1,2}\b')
   
   print(shortword.sub('', text))
   ```
   
   ```python
   was wondering anyone out there could enlighten this car.
   ```

---

### 2.4 정규 표현식(Regular Expression)

얻어낸 코퍼스에서 노이즈 데이터의 특징을 잡아낼 수 있다면, 정규 표현식을 통해 이를 제거하는 경우가 많다.
HTML 문서로부터 가져온 코퍼스라면 문서 여기저기에 HTML태크가 있다.
뉴스 기사를 크롤링했다면, 기사마다 게재 시간이 적혀져 있을 수 있다.
정규 표현식은 이러한 코퍼스 내에 계속해서 등장하는 글자들을 규칙에 기반하여 한 번에 제거하는 방식으로서 매우 유용하다.

---

### 3. 어간 추출(Stemming) & 표제어 추출(Lemmatization)

이번에는 정규화 기법 중 코퍼스에 있는 단어의 개수를 줄일 수 있는 기법인 표제어 추출과 어간 추출의 개념에 대해 알아보자.

이 두 작업이 갖고 있는 의미는 눈으로 봤을 때는 서로 다른 단어이지만, 하나의 단어로 일반화시킬 수 있다면 하나의 단어로 일반화시켜서 문서내의 단어 수를 줄이겠다는 것이다.
이러한 방법들은 단어의 빈도수를 기반으로 문제를 풀고자 하는 BoW(Bag of Words) 표현을 사용하는 NLP문제에서 주로 사용된다.
NLP에서 전처리, 더 정확히는 정규화의 지향점은 언제나 갖고 있는 코퍼스로 부터 복잡성을 줄이는 일이다.

---

### 3.1 표제어 추출(Lemmatization)

표제어(Lemma)는 한글로 '표제어' 또는 '기본 사전형 단어' 정도의 의미를 갖는다.
표제어 추출은 단어들로 부터 표제어를 찾아가는 과정이다.
표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단한다.
예를 들어서 am, are, is은 서로 다른 철자를 가지고 있지만, 그 뿌리 단어는 be 라고 볼 수 있다.
이때, 이 단어들의 표제어를 be라고 할 수 있다.

표제어 추출을 하는 가장 섬세한 방법은 단어의 형태학적 파싱을 먼저 진행하는 것이다.
형태소란 '의미를 가진 가장 단위'를 뜻한다.
그리고 형태학(morphology)이란, 형태소로부터 단어들을 만들어가는 학문을 뜻한다.

형태소는 두 가지 종류가 존재한다.

1. 어간(stem): 단어의 의미를 담고 있는 단어의 핵심 부분,
2. 접사(affix): 단어에 추가적인 의미를 주는 부분.

형태학적 파싱은 이 두 가지 구성 요소를 분리하는 작업을 지칭한다.
가령 cats라는 단어에 대해 형태학적 파싱을 수행한다면, 형태학적 파싱은 결과로 cat(어간)과 -s(접사)로 분리한다.
그런다고 꼭 두 가지로 분리되지 않는 경우도 있다.
단어 fox는 형태학적 파싱을 하더라도 더 이상 분리할 수 없다.
fox는 독립적인 하나의 형태소이기 때문이다.

NLTK에서 표제어 추출을 위한 도구인 WordNetLemmatizer를 지원한다.

```python
from nltk.stem import WordNetLemmatizer
n=WordNetLemmatizer()
words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']

print([n.lemmatize(w) for w in words])
```

```python
['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']
```

어간 추출에 대해서 배우고, 같은 입력에 대한 결과를 비교해보면 알겠지만 표제어 추출은 어간 추출과 달리 단어의 형태가 적절히 보존되는 양상을 보이는 특징이 있다.
하지만 그럼에도 위의 결과에서는 dy나 ha와 같이 의미를 알 수 없는 적절하지 못한 단어를 출력하고 있다.
이는 표제어 추출가 본래 단어의 품사 정보를 알아야만 정확한 결과를 얻을 수 있기 때문이다.

WordNetLemmatizer는 입력으로 단어가 동사 품사라는 사실을 알려줄 수 있다.
즉, dies와 watched, has거 문장에서 동사로 쓰였다는 것을 알려준다면 표제어 추출기는 품사의 정보를 보존하면서 정확한 표제어를 출력하게 된다.

```python
n.lemmatize('dies', 'v')
```

```python
'die'
```

```python
n.lemmatize('watched', 'v')
```

```python
'watch'
```

```python
n.lemmatize('has', 'v')
```

```python
'have'
```

어간 추출에 대해 말하기 전에, 표제어 추출과 어간 추출의 차이에 대해 먼저 알아보자.
표제어 추출은 문맥을 고려하며, 수행했을 때의 결과는 해당 단어의 품사 정보를 보존한다. (POS 태그를 보존한다고도 할 수 있다.)

하지만, 어간 추출을 수행한 결과는 품사 정보가 보존되지 않는다. (POS 태그를 고려하지 않는다.)
더 정확하게 말하자면, 어간 추출을 한 결과는 사전에 존재하지 않는 단어일 경우가 많다.

---

### 3.2 어간 추출(Stemming)

어간(Stem)을 추출하는 작업을 어간 추출(Stemming)이러고 한다.
어간 추출은 형태학적 분석을 단순화한 버전이라고 할 수도 있고, 정해진 규칙만 보고 단어의 어미를 자르는 어림짐작의 작업이라고도 할 수 있다.
다시 말해, 이 작업은 섬세한 작업이 아니기에, 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있다.

어간 추출 알고리즘 중 하나인 포터 알고리즘(Porter Algorithm)에 아래의 예문을 입력한다고 가정해보자.

input: This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.

```python
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
s = PorterStemmer()
text="This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes."
words=word_tokenize(text)

print(words)
```

```python
['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', "'s", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']
```

```python
print([s.stem(w) for w in words])
```

```python
['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', "'s", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']
```

위의 알고리즘의 결과에는 사전에 없는 단어들도 포함되어 있다.
위의 어간 추출은 단순 규칙에 기반하여 이루어지기 때문이다.

포터 알고리즘의 어간 추출은 이런 규칙들을 갖는다.

1. ALIZE → AL
2. ANCE → 제거
3. ICAL → IC

```python
words=['formalize', 'allowance', 'electricical']
print([s.stem(w) for w in words])
```

```python
['formal', 'allow', 'electric']
```

위의 규칙에 따라서 다음과 같은 결과가 나오게 된다.

- formalize → formal

- allowance → allow

- electricical → electric

  *Porter 알고리즘의 상세 규칙은 마틴 포터의 홈페이지에서 확인할 수 있다.

어간 추출 속도는 표제어 추출보다 일반적으로 빠른데, 포터 어간 추출기는 정밀하게 설계되어 정확도가 높아 영어 NLP에서 어간 추출을 하고자 한다면 포터 알고리즘이 가장 준수한 선택이다.
NLTK에서는 포터 알고리즘 외에도 랭커슽 스태머(Lancaster Stemmer) 알고리즘을 지원한다.
포터 알고리즘과 랭커스터 스태머 알고리즘으로 각각 어간 추출을 진행했을 때의 이 둘의 결과를 비교해보자.

```python
from nltk.stem import PorterStemmer
s=PorterStemmer()
words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']

print([s.stem(w) for w in words])
```

```python
['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']
```

```python
from nltk.stem import LancasterStemmer
l=LancasterStemmer()
words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']

print([l.stem(w) for w in words])
```

```python
['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']
```

동일한 단어들의 나열에 대해서 두 스태머는 전혀 다른 결과를 보여준다.
두 스태머 알고리즘은 서로 다른 알고리즘을 사용하기 때문이다.
그렇기에 이미 알려진 알고리즘을 사용할 때는, 사용하고자 하는 코퍼스에 스태머를 적용해보고 어떤 스태머가 해당 코퍼스에 적합한지 판단 후 에 사용해야 한다.

이런 규칙에 기반한 알고리즘은 종종 제대로 된 일반화를 수행하지 못 할 수 있습니다.
어간 추출을 하고나서 일반화가 지나치게 되거나 또는 덜 되거나 하는 경우이다.
예를 들어 포터 알고리즘에서 organization을 어간 추출했을 때의 결과를 봐보자.

- organization → organ

organization과 organ은 완전히 다른 단어 임에도 organization에 대해서 어간 추출을 수행한 결과로 organ이라는 단어나 나왔다.
organ에 대해서 어간 추출을 한다고 하더라도 결과는 organ이 되기에, 두 단어에 대해서 어간 추출을 한다면 동일한 어간을 갖게된다.
이는 어간 추출의 목적에는 맞지 않는다.

마지막으로 같은 단어에 대해서 포제어 추출과 어간 추출을 각각 수행했을 때, 결과에서 어떤 차이가 있는지 간단한 예를 봐보자.

- Stemming
  am → am
  the going → the go
  having → hav
- Lemmatization
  am → be
  the going → the going
  having → have

---

### 3.3 한국어에서의 어간 추출

한국어는 아래의 표와 같이 5언 9품사의 구조를 가지고 있다.

|   언   |        품사        |
| :----: | :----------------: |
|  체언  | 명사, 대명사, 수사 |
| 수식언 |    관형사, 부사    |
| 관계언 |        조사        |
| 독립언 |       감탄사       |
|  용언  |    동사, 형용사    |

이 중 용언에 해당하는 '동사'와 '형용사'는 어간과 어미(ending)의 결합으로 구성된다.
용언이라고 언급하는 부분은 전부 동사와 형용사를 포함하여 언급하는 개념이다.

1. 활용(conjugation)
   활용은 한국어에서만 가지는 특징이 아니라, 인도유럽어(indo-european language)에서도 주로 볼 수 있는 언어적 특징 중 하나를 말하는 통칭적인 개념이다.

   활용이란 용언의 어간이 어미를 가지는 일을 말한다.

   - 어간(stem): 용언(동사, 형용사)를 활용할 때, 원칙적으로 모양이 변하지 않는 부분.
     활용에서 어미에 선행하는 부분. 때론 어간의 모양도 바뀔 수 있음
     Ex) 긋다, 긋고, 그어서, 그어라
   - 어미(ending): 용언의 어간 뒤에 붙어서 활용하면서 변하는 부분이며, 여러 문법적 기능을 수행.

   활용은 어간이 어미를 취할 때, 어간의 모습이 일정하다면 규칙 활용, 어간이나 어미의 모습이 변하는 불규칙 활용으로 나뉜다.

2. 규칙 활용
   규칙 활용은 어간이 어미를 취할 때, 어간의 모습이 일정하다.
   아래의 예제는 어간과 어미가 합쳐질 때, 어간의 형태가 바뀌지 않음을 보여준다.

   - 잡/어간 + 다/어미

   이 경우에는 어간이 어미가 붙기전의 모습과 어미가 붙은 후의 모습이 같으므로, 규칙 기반으로 어미를 단순히 분리해주면 어간 추출이 된다.

3. 불규칙 활용
   불규칙 활용은 어간이 어미를 취할 때 어간의 모습이 바뀌거나, 취하는 어미가 특수한 어미일 경우를 말한다.

   예를 들어 ‘듣-, 돕-, 곱-, 잇-, 오르-, 노랗-’ 등이 ‘듣/들-, 돕/도우-, 곱/고우-, 잇/이-, 올/올-, 노랗/노라-’와 같이 어간의 형식이 달라지는 일이 있거나 ‘오르+ 아/어→올라, 하+아/어→하여, 이르+아/어→이르러, 푸르+아/어→푸르러’와 같이 일반적인 어미가 아닌 특수한 어미를 취하는 경우 불규칙활용을 하는 예에 속한다.

   이 경우에는 어간이 어미가 붙는 과정에서 어간의 모습이 바뀌었으므로 단순한 분리만으로 어간 추출이 되지 않고 좀 더 복잡한 규칙을 필요로 한다.

   *불규칙 활용의 예: https://namu.wiki/w/한국어/불규칙%20활용