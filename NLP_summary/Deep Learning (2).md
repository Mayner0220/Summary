# Deep Learning (2)

Source: https://wikidocs.net/22882, https://wikidocs.net/24958, https://wikidocs.net/24987, https://wikidocs.net/36033, https://wikidocs.net/37406, https://wikidocs.net/61374, https://wikidocs.net/61375, https://wikidocs.net/32105, https://wikidocs.net/38861, https://wikidocs.net/49071, https://wikidocs.net/45609

---

### 4. 과적합(Overfitting)을 막는 방법

학습 데이터에 모델이 과적합되는 현상은 모델의 성능을 떨어트리는 주요 이슈이다.
모델이 과적합되면 훈련 데이터에 대한 정확도는 높을지라도, 새로운 데이터, 즉, 검증 데이터나 테스트 데이터에 대해서는 제대로 동작하지 않는다.
이는 모델이 학습 데이터를 불필요할정도로 과하게 암기하여 훈련 데이터에 포함된 노이즈까지 학습한 상태라고 해석할 수 있다.
이번에는 모델의 과적합을 막을 수 있는 여러가지 방법에 대해서 논의해보자.

---

### 4.1 데이터의 양을 늘리기

모델은 데이터의 양이 적을 경우, 해당 데이터의 특정 패턴이나 노이즈까지 쉽게 암기하기 되므로 과적합 현상이 발생할 확률이 늘어난다.
그렇기 때문에 데이터의 양을 늘릴 수록 모델은 데이터의 일반적인 패턴을 학습하여 과적합을 방지할 수 있다.

만약, 데이터의 양이 적을 경우에는 의도적으로 기존의 데이터를 조금씩 변혀앟고 추가하여 데이터의 양을 늘리기도 하는데 이를 데이터 증식 또는 증강(Data Augmentation)이라고 한다.
이미지의 경우에는 데이터 증식이 많이 사용되는데 이미지를 돌리거나 노이즈를 추가하고, 일부분 수정하는 등으로 데이터를 증식시킨다.

---

### 4.2 모델의 복잡도 줄이기

인공 신경망의 복잡도는 은닉층(hidden layer)의 수나 매개변수의 수 등으로 결정된다.
과적합 현상이 포착되었을 때, 인공 신경망 모델에 대해서 할 수 있는 한 가지 조치는 인공 신경망의 복잡도를 줄이는 것이다.

- 인공 신경망에서는 모델에 있는 매개변수들의 수를 모델의 수용력(capacity)이라고 하기도 한다.

---

### 4.3 가중치 규제(Regularization) 적용하기

복잡한 모델이 간단한 모델보다 과적합될 가능성이 높다.
그리고 간단한 모델은 적은 수의 매개변수를 가진 모델을 말한다.
복잡한 모델을 좀 더 간단하게 하는 방법으로 가중치 규제가 있다.

- L1 규제: 가중치 w들의 절대값 합계를 비용 함수에 추가한다. (L1 노름이라고도 한다.)
- L2 규제: 모든 가중치 w들의 제곱합을 비용 함수에 추가한다. (L2 노름이라고도 한다.)

L1 규제는 기존의 비용 함수에 모든 가중치에 대해서 λ∣w∣를 더한 값을 바용함수로 하고, L2 규제는 기존의 비용 함수에 모든 가중치에 대해서 1/2λw^2 를 더한 값을 비용 함수로 한다.
λ는 규제의 강도를 정하는 하이퍼파라미터이다.
λ가 크다면 모델이 훈련 데이터에 대해서 적합한 매개 변수를 찾는 것보다 규제를 위해 추가된 항들을 작게 유지하는 것을 우선한다는 의미가 된다.

이 두 식 모두 비용 함수를 최소화하기 위해서는 가중치 w들의 값이 작아져야 한다는 특징이 있다.
L1 규제로 예를 들어보자.
L1 규제를 사용하면 비용 함수가 최소가 되게 하는 가중치와 편향을 찾는 동시에 가중치들의 절대값의 합도 최소가 되어야한다.
이렇게 되면, 가중치 w의 값들은 0 또는 0에 가까이 작아져야 하므로 어떤 특성들은 모델을 만들 때 거의 사용되지 않게 된다.

예를 들어 H(x)=w1x1+w2x2+w3x3+w4x4라는 수식이 있다고 해보자.
여기에 L1 규제를 사용하였더니, w3의 값이 0이 되었다고 해봅시다. 이는 x3 특성은 사실 모델의 결과에 별 영향을 주지 못하는 특성임을 의미한다.

L2 규제는 L1 규제와는 달리 가중치들의 제곱을 최소화하므로 w의 값이 완전히 0이 되기보다는 0에 가까워지기는 경향을 뛴다.
L1 규제는 어떤 특성들이 모델에 영향을 주고 있는지를 정확히 판단하고자 할 때 유용하다.
만약, 이런 판단이 필요없다면 경험적으로는 L2 규제가 더 잘 동작하므로 L2 규제를 더 권장한다.
인공 신경망에서 L2 규제는 가중치 감쇠(weight decay)라고도 부른다.

---

### 4.4 드롭아웃(Dropout)

드롭아웃은 학습 과정에서 신경망의 일부를 사용하지 않는 방법이다.
예를 들어 드롭아웃의 비율을 0.5로 한다면 학습 과정마다 랜덤으로 절반의 뉴런을 사용하지 않고, 절반의 뉴런만을 사용한다.

![img](https://wikidocs.net/images/page/60751/%EB%93%9C%EB%A1%AD%EC%95%84%EC%9B%83.PNG)

드롭아숭은 신경망 학습 시에만 사용하고, 예측 시에는 사용하지 않는 것이 일반적이다.
학습 시에 인공 신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지해주고, 매번 랜덤 선택으로 뉴런들을 사용하지 않으므로 서로 다른 신경망들을 앙상블하여 사용하는 것 같은 효과를 내어 과적합을 방지한다.

케라스에서는 다음과 같은 방법으로 드롭아웃을 모델에 추가할 수 있다. 

```python
model = Sequential()
model.add(Dense(256, input_shape=(max_words,), activation='relu'))
model.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%
model.add(Dense(num_classes, activation='softmax'))
```

---

### 5. 기울기 소실(Gradient Vanishing)과 폭주(Exploding)

깊은 인공 신경망을 학습하다보면 역전파 과정에서 입력층으로 갈 수록 기울기(Gradient)가 점차적으로 작아지는 현상이 발생할 수 있다.
입력층에 가까운 충돌에서 가중치들이 업데이트가 제대로 되지 않으면 결국 최적의 모델을 찾을 수 없게 된다.
이를 기울기 소실(Gradient)(Gradient)이라고 한다.

반대의 경우도 있다.
기울기가 점차 커지더니 가중치들이 비정상적으로 큰 값이 되면서 결국 발산되기도 한다.
이를 기울기 폭주(Gradient Exploding)이라고 하며, 뒤에서 배울 순환 신경망(Recurrent Neural Network, RNN)에서 발생할 수 있다.

---

### 5.1 ReLU와 ReLU의 변형들

앞에서 배운 내용을 간단하게 복습해보자.
시그모이드 함수를 사용하면 입력의 절대값이 클 경우에 시그모이드 함수의 출력값이 0 또는 1에 수렴하면서 기울기가 0에 가까워진다.
그래서 역전파 과정에서 전파 시킬 기울기가 점차 사라져서 입력층 방향으로 갈 수록 제대로 역전파가 되지 않는 기울기 소실 문제가 발생할 수 있다.

기울기 소실을 완화하는 가장 간단한 방법은 은닉층의 활성화 함수로 시그모이드나 하이퍼볼릭탄젠트 함수 대신에 ReLU나 ReLU의 변형 함수와 같은 Leaky ReLU를 사용하는 것이다.

- 은닉층에서는 시그모이드 함수를 사용하지 마세요.
- Leaky ReLU를 사용하면 모든 입력값에 대해서 기울기가 0에 수렴하지 않아 죽은 ReLU 문제를 해결한다.
- 은닉층에서는 ReLU나 Leaky ReLU와 같은 ReLU 함수의 변형들을 사용하자.

---

### 5.2 그래디언트 클리핑(Gradient Clipping)

그래디언트 클리핑은 말 그대로  기울기 값을 자르는 것을 의미한다.
기울기 폭주를 막기 위해 임계값을 넘지 않도록 값을 자른다.
다시 말해서 임계치만큼 크기를 감소시킨다.
이는 RNN에서 유용하다.
RNN은 BPTT에서 시점을 역해와면서 기울기를 구하는데, 이때 기울기가 너무 커질 수 있기 때문이다.
케라스에서는 다음과 같은 방법으로 그래디언트 클리핑을 수행한다.

```python
from tensorflow.keras import optimizers
Adam = optimizers.Adam(lr=0.0001, clipnorm=1.)
```

---

### 5.3 가중치 초기화(Weight initialization)

같은 모델을 훈련시키더라도 가중치가 초기에 어떤 값을 가졌느냐에 따라서 모델의 훈련 결과가 달라지기도 한다.
다시 말해 가중치 초기화만 적절히 해줘도 기울기 소실 문제과 같은 문제를 완화시킬 수 있다.

1. 세이비어 초기화(Xavier Initialization)

   - 논문 : http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf

   2010년 세이비어 글로럿과 요슈아 벤지오는 가중치 초기화가 모델에 미치는 영향을 분석하여 새로운 초기화 방법을 제안했다.
   이 초기화 방법은 제안한 사람의 이름을 따서 세이비어 초기화(Xavier Initialization) 또는 글로럿 초기화(Glorot Initialization)라고 한다.

   이 방법은 균등 분포(Uniform Distribution) 또는 정규 분포(Normal Distribution)로 초기화 할 때 두 가지 경우로 나뉘며, 이전 층의 뉴런 개수와 다음 층의 뉴런 개수를 가지고 식을 세운다.
   이전 층의 뉴런의 개수를 
   $$
   n_{in}
   $$
   , 다음 층의 뉴런의 개수를 
   $$
   n_{out}
   $$
   이라고 해보자.

   글로럿과 벤지오의 논문에서는 균등 분포를 사용하여 가중치를 초기화할 경우 다음과 같은 균등 분포 범위를 사용하라고 한다.
   $$
   W \sim Uniform(-\sqrt{\frac{6}{ {n}_{in} + {n}_{out} }}, +\sqrt{\frac{6}{ {n}_{in} + {n}_{out} }})
   $$
   다시 말해 
   $$
   \sqrt{\frac{6}{ {n}_{in} + {n}_{out} }}
   $$
   를 m이라고 하였을 때, -m과 +m 사이의 균등 분포를 의미한다.

   정규 분포로 초기화할 경우에는 평균이 0이고, 표준 편차 σ가 다음을 만족하도록 한다.
   $$
   σ=\sqrt{\frac { 2 }{ { n }_{ in }+{ n }_{ out } } }
   $$
   세이비어 초기화는 여러 층의 기울기 분산 사이에 균형을 맞춰서 특정 층이 너무 주목을 받거나 다른 층이 뒤쳐지는 것을 막는다.
   그런데 세이비어 초기화는 시그모이드 함수나 하이퍼볼릭 탄젠트 함수와 같은 S자 형태인 활성화 함수와 함께 사용할 경우에는 좋은 성능을 보이지만, ReLU와 함께 사용할 경우에는 성능이 좋지 않다.
   ReLU 함수 또는 ReLU의 변형 함수들을 활성화 함수로 사용할 경우에는 다른 초기화 방법을 사용하는 것이 좋은데, 이를 He 초기화(He initialization)라고 한다.

2. He 초기화(He initialization)

   - 논문 : https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf

   He 초기화는 세이비어 초기화와 유사하게 정규 분포와 균등 분포 두 가지 경우로 나뉜다.
   다만, He 초기화는 세이비어 초기화와 다르게 다음 층의 뉴런의 수를 반영하지 않는다.
   전과 같이 이전 층의 뉴런의 개수를 
   $$
   n_{in}
   $$
   이라고 해보자.

   He 초기화는 균등 분포로 초기화 할 경우에는 다음과 같은 균등 분포 범위를 가지도록 한다.
   $$
   σ=\sqrt{\frac { 2 }{ { n }_{ in } } }
   $$

   - 시그모이드 함수나 하이퍼볼릭탄젠트 함수를 사용할 경우에는 세이비어 초기화 방법이 효율적이다.
   - ReLU 계열 함수를 사용할 경우에는 He 초기화 방법이 효율적이다.
   - ReLU + He 초기화 방법이 좀 더 보편적이다.

---

### 5.4 배치 정규화(Batch Normalization)

ReLU 계열의 함수와 He 초기화를 사용하는 것만으로도 어느 정도 기울기 소실과 폭주를 완화시킬 수 있지만, 이 두 방법을 사용하더라도 훈련 중에 언제든 다시 발생할 수 있따.
기울기 소실이나 폭주를 예방하는 또 다른 방법은 배치 정규화(Batch Normalization)이다.
배치 정규화는 인공 신경망의 각 층에 들어가는 입력을 평균과 분산으로 정규화하여 학습을 효율적으로 만든다.

1. 내부 공변량 변화(Internal Covariate Shift)
   배치 정규화를 이해하기 위해서는 내부 공변향 변화(Internal Covariate Shift)를 이해할 필요가 있다.
   내부 공변향 변화란 학습 과정에서 층 별로 입력 데이터 분포가 달라지는 현상을 말한다.
   이전 층들의 학습에 의해 이전 층의 가중치 값이 바뀌게 되면, 현재 층에 전달되는 입력 데이터의 분포가 현재 층이 학습했던 시점의 분포와 차이가 발생한다.
   배치 정규화를 제안한 논문에서는 기울기 소실/폭주 등의 딥러닝 모델의 불안전성ㅇ이 층마다 입력의 분포가 달라지기 때문이라고 주장한다.

   - 공변량 변화는 훈련 데이터의 분포와 테스트 데이터의 분포가 다른 경우를 의미한다.
   - 내부 공변량 변화는 신경망 층 사이에서 발생하는 입력 데이터의 분포 변화를 의미한다.

2. 배치 정규화(Batch Normalization)
   배치 정규화는 표현 그대로 한 번에 들어오는 배치 단위로 정규화하는 것을 말한다.
   배치 정규화는 각 층에서 활성화 함수를 통과하기 전에 수행된다.
   배치 정규화를 요약하면 다음과 같다.
   입력에 대해 평균을 0으로 만들고, 정규화를 한다.
   그리고 정규화된 데이터에 대해서 스케일과 시프트를 수행한다.
   이 때 두 개의 매개변수 γ와 β를 사용하는데, γ는 스케일을 위해 사용하고, β는 시프트를 하는 것에 사용하며 다음 레이어에 일정한 범위의 값들만 전달되게 한다.

   배치 정규화의 수식은 다음과 같다.
   아래에서 BN은 배치 정규화를 의미한다.

   - Input: 미니 배치 
     $$
     B = \{{x}^{(1)}, {x}^{(2)}, ..., {x}^{(m)}\}
     $$

   - Output: 
     $$
     y^{(i)} = BN_{γ, β}(x^{(i)})
     $$

   $$
   μ_{B} ← \frac{1}{m} \sum_{i=1}^{m} x^{(i)} \text{ # 미니 배치에 대한 평균 계산}
   $$

   $$
   σ^{2}_{B} ← \frac{1}{m} \sum_{i=1}^{m} (x^{(i)} - μ_{B})^{2}\text{ # 미니 배치에 대한 분산 계산}
   $$

   $$
   \hat{x}^{(i)} ← \frac{x^{(i)} - μ_{B}}{\sqrt{σ^{2}_{B}+ε}}\text{ # 정규화}
   $$

   $$
   y^{(i)} ← γ\hat{x}^{(i)} + β = BN_{γ, β}(x^{(i)}) \text{ # 스케일 조정(γ)과 시프트(β)를 통한 선형 연산}
   $$

   - m은 미니 배치에 있는 샘플의 수
   - μB는 미니 배치 B에 대한 평균
   - σB는 미니 배치 B에 대한 표준편차
   - x^(i)은 평균이 0이고 정규화 된 입력 데이터
   - ε은 σ2가 0일 때, 분모가 0이 되는 것을 막는 작은 양수. 보편적으로 10^(−5)
   - γ는 정규화 된 데이터에 대한 스케일 매개변수로 학습 대상
   - β는 정규화 된 데이터에 대한 시프트 매개변수로 학습 대상
   - y(i)는 스케일과 시프트를 통해 조정한 BN의 최종 결과

   배치 정규화는 학습 시 배치 단위의 평균과 분산들을 차례대로 받아 이동 평균과 이동 분산을 저장해놓았다가 테스트 할 때는 해당 배치의 평균과 분산을 구하지 않고 구해놓았던 평균과 분산으로 정규화를 한다.

   - 배치 정규화를 사용하면 시그모이드 함수나 하이퍼볼릭탄젠트 함수를 사용하더라도 기울기 소실 문제가 크게 개선된다.
   - 가중치 초기화에 훨씬 덜 민감해진다.
   - 훨씬 큰 학습률을 사용할 수 있어 학습 속도를 개선시킨다.
   - 미니 배치마다 평균과 표준편차를 계산하여 사용하므로 훈련 데이터에 일종의 잡음 주입의 부수 효과로 과적합을 방지하는 효과도 냅니다. 다시 말해, 마치 드롭아웃과 비슷한 효과를 냅니다. 물론, 드롭 아웃과 함께 사용하는 것이 좋다.
   - 배치 정규화는 모델을 복잡하게 하며, 추가 계산을 하는 것이므로 테스트 데이터에 대한 예측 시에 실행 시간이 느려집니다. 그래서 서비스 속도를 고려하는 관점에서는 배치 정규화가 꼭 필요한지 고민이 필요하다.
   - 배치 정규화의 효과는 굉장하지만 내부 공변량 변화때문은 아니라는 논문도 있다.
     https://arxiv.org/pdf/1805.11604.pdf

3. 배치 정규화의 한계
   배치 정규하는 뛰어난 방법이지만 몇 가지 한계가 존재한다.

   1. 미니 배치 크기에 의존적이다.
      배치 정규화는 너무 작은 배치 크기에서는 잘 작동하지 않을 수 있다.
      단적으로 배치 크기를 1로 하게되면 분산은 0이 된다.
      작은 미니 배치에서는 배치 정규화의 효과가 극단적으로 작용되어 훈련에 악영향을 줄 수 있다.
      배치 정규화를 적용할 때는 작은 미니 배치보다는 크기가 어느 정도 되는 미니 배치에서 하는 것이 좋다.
      이 처럼 배치 정규화는 배치 크기에 의존적인 면이 있다.
   2. RNN에 적용하기 어렵다.
      뒤에서 배우지만, RNN은 각 시점(time step)마다 다른 통계치를 가진다.
      이는 RNN에 배치 정규화를 적용하기 위한 몇 가지 논문이 제시되어 있지만, 여기서는 이를 소개하는 대신 배치 크기(layer normalization)라는 방법을 소개하고자 한다.

---

### 5.5 층 정규화(Layer Normalization)

층 정규화를 이해하기에 앞서 배치 정규화를 시각화해보자.
다음은 m이 3이고, 특성의 수가 4일 때의 배치 정규화를 보여준다.
미니 배치란 동일한 특성(feature) 개수들을 가진 다수의 샘플들을 의미함을 상기하자.

![img](https://wikidocs.net/images/page/61375/%EB%B0%B0%EC%B9%98%EC%A0%95%EA%B7%9C%ED%99%94.PNG)

반면, 층 정규화는 다음과 같다.

![img](https://wikidocs.net/images/page/61375/%EC%B8%B5%EC%A0%95%EA%B7%9C%ED%99%94.PNG)