오차 함수로는 크로스 엔트로피 함수를 사용



# Machine Learning (2)

Source: https://wikidocs.net/21669, https://wikidocs.net/21679, https://wikidocs.net/32012, https://wikidocs.net/21670, https://wikidocs.net/22881, https://wikidocs.net/35821, https://wikidocs.net/37001, https://wikidocs.net/35476

---

### 5. 다중 입력에 대한 실습

독립 변수 x가 2개 이상인 선형 회귀와 로지스틱 회귀에 대해서 배워도록 하자.
비용 함수와 옵티마이저 등을 사용하는 원리는 다양하다.

---

### 5.1 다중 선형 회귀

DL에서는 대부분의 입력들은 독립 변수가 2개 이상이다.
이는 모델을 직접 코딩하는 관점에서 입력 벡터의 차원이 2 이상이라고 할 수 있다.
y를 결정하는데 있어 독립 변수가 3개인 선형 회귀를 풀어보자.
중간 고사, 기말 고사, 그리고 추가 점수를 어떤 공식을 통해 최종 점수를 계산한 데이터가 있다.

| Midterm(x1) | Final(x2) | Added point(x3) | Score($1000)(y) |
| :---------: | :-------: | :-------------: | :-------------: |
|     70      |    85     |       11        |       73        |
|     71      |    89     |       18        |       82        |
|     50      |    80     |       20        |       72        |
|     99      |    20     |       10        |       57        |
|     50      |    10     |       10        |       34        |
|     20      |    99     |       10        |       58        |
|     40      |    50     |       20        |       56        |

이 경우 가설은 다음과 같다.
$$
H(X) = {W_1x_1 + W_2x_2 + W_3x_3 + b}
$$
위의 데이터 중 상위 5개의 데이터만 훈련에 사용하고, 나머지 2개는 테스트에 사용해보자.

```python
import numpy as np
from tensorflow.keras.models import Sequential # 케라스의 Sequential()을 임포트
from tensorflow.keras.layers import Dense # 케라스의 Dense()를 임포트
from tensorflow.keras import optimizers # 케라스의 옵티마이저를 임포트

X=np.array([[70,85,11],[71,89,18],[50,80,20],[99,20,10],[50,10,10]]) # 중간, 기말, 가산점
# 입력 벡터의 차원은 3입니다. 즉, input_dim은 3입니다.
y=np.array([73,82,72,57,34]) # 최종 성적
# 출력 벡터의 차원은 1입니다. 즉, output_dim은 1입니다.

model=Sequential()
model.add(Dense(1, input_dim=3, activation='linear'))
sgd=optimizers.SGD(lr=0.00001)
# 학습률(learning rate, lr)은 0.00001로 합니다.
model.compile(optimizer=sgd ,loss='mse',metrics=['mse'])
# sgd는 경사 하강법을 의미.
# 손실 함수(Loss function)은 평균제곱오차 mse를 사용합니다.
model.fit(X,y, batch_size=1, epochs=2000, shuffle=False)
# 주어진 X와 y데이터에 대해서 오차를 최소화하는 작업을 2,000번 시도합니다.
```

앞서 실습한 선형 회귀 코드와 거의 동일한데 달라진 점은 입력의 차원이 3으로 바뀌면서, input_dim의 인자값이 3으로 바뀌었다는 점이다.
이는 입력 벡터의 차원이 3임을 의미한다.

```python
Epoch 1/2000
5/5 [==============================] - 0s 86ms/step - loss: 18534.6645 - mean_squared_error: 18534.6645
... 중략 ...
Epoch 2000/2000
5/5 [==============================] - 0s 2ms/step - loss: 0.0363 - mean_squared_error: 0.0363
```

아직 오차(loss)가 줄어들 여지가 있지만, 여기서는 이 정도에서 예측 작업을 해보자.

```python
print(model.predict(X))
```

```python
[[73.15294 ]
 [81.98001 ]
 [71.93192 ]
 [57.161617]
 [33.669353]]
```

기존 데이터를 주고 예측을 시켰을 때 아직 정확하게 예측한 건 아니지만 어느 정도 실제값에 근접한 예측을 하는 것을 볼 수 있다.

이제 훈련할 때 사용하지 않았던 데이터를 가지고 예측 작업을 수행해보자.

```python
X_test=np.array([[20,99,10],[40,50,20]]) # 각각 58점과 56점을 예측해야 합니다.
print(model.predict(X_test))
```

```python
[[58.08134 ]
 [55.734634]]
```

---

### 5.2 다중 로지스틱 회귀

y를 결정하는데 있어 독립 변수 x가 2개인 로지스틱 회귀를 풀어보자.
꽃받침(Sepal)의 길이와 꽃잎(Petal)의 길이와 해당 꽃이 A인지 B인지가 적혀져 있는 데이터가 있을 때, 새로 조사란 꽃받침의 길이와 꽃잎의 길이로부터 무슨 꽃인지 예측할 수 있는 모델을 만들고자 한다면 이 때 독립 변수 x는 2개가 된다.

| SepalLengthCm(x1) | PetalLengthCm(x2) | Species(y) |
| :---------------: | :---------------: | :--------: |
|        5.1        |        3.5        |     A      |
|        4.7        |        3.2        |     A      |
|        5.2        |        1.8        |     B      |
|         7         |        4.1        |     A      |
|        5.1        |        2.1        |     B      |

이 경우 가설은 다음과 같다.
$$
H(X) = sigmoid({W_1x_1 + W_2x_2 + b})
$$
이제 독립 변수가 2개인 좀 더 간단한 예를 들고, 이를 케라스로 구현해봅시다.
OR 게이트는 0 또는 1의 값을 입력으로 받는데, 두 개의 입력 x1, x2 중 하나라도 1이면 출력값 y가 1이 되고 두 개의 입력이 0인 경우에만 출력값이 0이 되는 게이트이다.
로지스틱 회귀를 통해 OR 게이트를 구현해보자.

```python
import numpy as np
X=np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
# 입력 벡터의 차원은 2입니다. 즉, input_dim은 2입니다.
y=np.array([0, 1, 1, 1])
# 출력 벡터의 차원은 1입니다. 즉, output_dim은 1입니다.
```

```python
from tensorflow.keras.models import Sequential # 케라스의 Sequential()을 임포트
from tensorflow.keras.layers import Dense # 케라스의 Dense()를 임포트
from tensorflow.keras import optimizers # 케라스의 옵티마이저를 임포트

model=Sequential()
model.add(Dense(1, input_dim=2, activation='sigmoid')) # 이제 입력의 차원은 2입니다.
model.compile(optimizer='sgd' ,loss='binary_crossentropy',metrics=['binary_accuracy'])
# sgd는 경사 하강법을 의미.
# 손실 함수(Loss function)는 binary_crossentropy(이진 크로스 엔트로피)를 사용합니다.
model.fit(X,y, batch_size=1, epochs=800, shuffle=False)
# 주어진 X와 y데이터에 대해서 오차를 최소화하는 작업을 800번 시도합니다.
```

앞서 실습한 로지스틱 회귀 코드와 거의 동일한데 달라진 점은 입력의 차원이 2로 바뀌면서, input_dim의 인자값이 2로 바뀌었다는 점이다.
이는 입력 벡터의 차원이 2임을 의미한다.

```python
Epoch 1/800
4/4 [==============================] - 0s 46ms/step - loss: 0.9076 - binary_accuracy: 0.5000
... 중략 ...
Epoch 800/800
4/4 [==============================] - 0s 2ms/step - loss: 0.2222 - binary_accuracy: 1.0000
```

정확도는 100%가 나오고 있으므로 800회 정도로 학습을 멈추고 시그모이드 함수의 각 입력값에 대해서 출력값이 0.5보다 크고 작은지를 확인해보자.

```python
print(model.predict(X))
```

```python
[[0.45521256]
 [0.84107596]
 [0.8577089 ]
 [0.97447586]]
```

입력이 둘 다 0, 0인 경우를 제외하고 나머지 3개의 입력 쌍에 대해서는 전부 값이 0.5를 넘는 것을 볼 수 있다.

---

### 5.4 인공 신경망 다이어그램

다중 로지스틱 회귀를 뒤에서 배우게 되는 인공 신경망을 형태로 표현하면 다음과 같다.
아직 인공 신경망을 배우지 않았음에도 이렇게 다이어그램으로 표현해보는 이유는 로지스틱 회귀를 일종의 인공 신경망 구조로 해석해도 무방함을 보여주기 위함이다.
$$
y = sigmoid(W_1x_1 + W_2x_2 + W_3x_3 + ... + W_nx_n + b) = σ(W_1x_1 + W_2x_2 + W_3x_3 + ... + W_nx_n + b)
$$
![img](https://wikidocs.net/images/page/35821/multiplelogistic_regression.PNG)

---

### 6. 벡터와 행렬 연산

앞서 독립 변수 x가 2개 이상인 선형 회귀와 로지스틱 회귀에 대해서 배웠다.
그런데 소프트맥스 회귀에서는 종속 변수 y의 종류도 3개 이상이 되면서 더욱 복잡해진다.
그리고 이러한 식들이 겹겹이 누적되면 인공 신경망의 개념이 된다.

케라스는 사용하기가 편리해서 이런 고민을 할 일이 상대적으로 적지만, Numpy나 텐서플로우의 로우-레벨의 ML 개발을 하게 되면 각 변수들의 연산을 벡터와 행렬 연산으로 이해할 수 있어야 한다.
다시 말해 사용자가 데이터와 변수의 개수로부터 행렬의 크기, 더 나아가 텐서의 크기를 산정할 수 있어야 한다.
이번 챕터에서는 기본적인 벡터와 행렬 연산에 대해서 이해해보자.

---

### 6.1 벡터와 헹렬과 텐서

벡터는 크기와 방향을 가진 양이다.
숫자가 나열된 형상이며 파이썬에서는 1차원 배열 또는 리스트로 표현한다.
반면, 행렬은 행과 열을 가지는 2차원 형상을 가진 구조이다.
파이썬에서는 2차원 배열로 표현한다.
가로줄 행(row)라고 하며, 세로줄을 열(column)이라고 한다.
3차원부터는 주로 텐서라고 부른다.
텐서는 파이썬에서는 3차원 이상의 배열로 표현한다.

---

### 6.2 텐서(Tensor)

인공 신경망은 복잡한 모델 내의 연산을 주로 행렬 연산을 통해 해결한다.
그런데 여기서 말하는 행렬 연산이란 단순히 2차원 배열을 통한 행렬 연산만을 의미하는 것이 아니다.
ML의 입/출력이 복잡해지면 3차원 텐서에 대한 이해가 필수로 요구된다.
예를 들어 인공 신경망 모델 중 하나인 RNN에서는 3차원 텐서에 대한 개념을 모르면 RNN을 이해하기가 쉽지 않다.

텐서를 설명하기 위한 아래의 모든 코드는 Numpy를 import했다고 가정하자.

```python
import numpy as np
```

1. 0차원 텐서
   스칼라는 하나의 실수값으로 이루어진 데이터를 말한다.
   또한 스칼라 값을 0차원 텐서라고 한다.
   차원을 영어로 Dimensionality라고 하므로 0D 텐서라고도 한다.

   ```python
   d=np.array(5)
   print(d.ndim) # 차원수 출력
   print(d.shape) # 텐서의 크기 출력
   ```

   ```python
   0
   () #크기가 없음
   ```

   Numpy의 ndim은 축의 개수를 출력하는데, 이는 텐서에서의 차원수와 동일하다.

2. 1차원 텐서
   숫자를 특정 순서대로 배열한 것을 벡터라고 한다.
   또한 벡터를 1차원 텐서라고 한다.
   주의할 점은 벡터의 차원과 텐서의 차원은 다른 개념이라는 점이다.
   아래의 예제는 4차원 벡터이지만, 1차원 텐서이다.
   1Dx 텐서라고도 한다.

   ```python
   d=np.array([1, 2, 3, 4])
   print(d.ndim)
   print(d.shape)
   ```

   ```python
   1
   (4,)
   ```

   벡터의 차원과 텐서의 차원의 정의로 인해 혼동할 수 있는데 벡터에서의 차원은 하나의 축에 차원들이 존재하는 것이고, 텐서에서의 차원은 축의 개수를 의미한다.

3. 2차원 텐서
   행과 열이 존재하는 벡터의 배열.
   즉, 행렬(matrix)을 2차원 텐서라고 한다.
   2D 텐서라고도 한다.

   ```python
   d=np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])
   print(d.ndim)
   print(d.shape)
   ```

   ```python
   2
   (3, 4)
   ```

   텐서의 크기를 보고 머리 속에 떠올릴 수 있으면 설계 시에 유용해야 한다.
   이게 어렵다면, 큰 단위부터 확장해나가며 생각하면 된다.
   위의 경우 3개의 커다란 데이터가 있는데, 그 각각의 커다란 데이터는 작은 데이터 4개로 이루어졌다고 생각할 수 있다.

   1차원 텐서를 벡터, 2차원 텐서를 행렬로 비유했는데 수학적으로 행렬릐 열을 열벡터라고 부르거나, 열벡터를 열행렬로 부르는 것과 혼동해서는 안된다.
   여기서 말하는 1차원 텐서와 2차원 텐서는 차원 자체가 달라야 한다.

4. 3차원 텐서
   행렬 또는 2차원 텐서를 단위로 한 번 더 배열하면 3차원 텐서라고 부른다.
   3D 텐서라고도 한다.
   사실 위에서 언급한 0차원 ~ 2차원 텐서는 각각 스칼라, 벡터, 행렬이라고 해도 무방하므로 3차원 이상의 텐서부터 본격적으로 텐서라고 부른다.
   조금 쉽게 말하면 데이터 사이언스 분야 한정으로 주로 3차원 이상의 배열을 탠서라고 부른다.
   그렇다면 3D 텐서는 적어도 여기서는 3차원 배열로 이해하면 된다.

   이 3차원 텐서의 구조를 이해하지 않으면, 복잡한 인공 신경망의 입/출력값을 이해하는 것이 쉽지 않다.
   개념 자체는 어렵지 않지만 반드시 알아야하는 개념이다.

   ```python
   d=np.array([
               [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [10, 11, 12, 13, 14]],
               [[15, 16, 17, 18, 19], [19, 20, 21, 22, 23], [23, 24, 25, 26, 27]]
               ])
   print(d.ndim)
   print(d.shape)
   ```

   ```python
   3
   (2, 3, 5)
   ```

   2개의 큰 데이터가 있는데, 그 각각은 3개의 더 작은 데이터로 구성되며, 그 3개의 데이터는 또한 더 작은 5개의 데이터로 구성되어져 있다.

   NLP에서 특히 자주 보게 되는 것이 이 3D 텐서이다.
   3D 텐서는 시퀀스 데이터(sequence data)를 표현할 때 자주 사용되기 때문이다.
   여기서 시퀀스 데이터는 주로 단어의 시퀀스를 의미하며, 시퀀스는 주로 문장이나 문서, 뉴스 기사등의 텍스트가 될 수 있다.
   이 경우 3D 텐서는 (samples, timesteps, word_dim)이 된다.
   또는 일괄로 처리하기 위해 데이터를 묶는 단위인 배치의 개념에 대해서 후에 배우게 되는데 (batch_size, timestep, word_dim)이라고도 볼 수 있다.

   samples/batch_size는 데이터의 개수, timesteps는 시퀀스의 길이, word_dim은 단어를 표현하는 벡터의 차원을 의미한다.
   더 상셍한 설명은 RNN에서 배우게 되지만, NLP에서 왜 3D 텐서의 개념이 사용되는지 간단한 예를 들어보자.

   - 문서1: I like NLP
   - 문서2: I like DL
   - 문서3: DL is AI

   이를 인공 신경망의 모델의 입력으로 사용하기 위해서는 각 단어를 벡터화해야 한다.
   단어를 벡터화하는 방버으로는 원-핫 인코딩이나 워드 임베딩이라는 방법이 대표적이나 워드 임베딩은 아직 배우지 않았으므로 원-핫 인코딩으로 모든 단어를 벡터화 해보자.

   | 단어 | One-hot vector |
   | ---- | -------------- |
   | I    | [1 0 0 0 0 0]  |
   | like | [0 1 0 0 0 0]  |
   | NLP  | [0 0 1 0 0 0]  |
   | DL   | [0 0 0 1 0 0]  |
   | is   | [0 0 0 0 1 0]  |
   | AI   | [0 0 0 0 0 1]  |

   그럼 기존에 있던 훈련 데이터를 모두 원-핫 벡터로 바꿔서 인공 신경망의 입력으로 한 꺼번에 사용한다고 하면 다음과 같다. (이렇게 훈련 데이터를 여러 개 묶어서 한 꺼번에 입력으로 사용하는 것을 배치(Batch)라고 한다.)

   [[[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0]],
   [[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]],
   [[0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]]]

   이는 (3, 3, 6)의 크기를 가지는 3D 텐서이다.

5. 그 이상의 텐서
   3차원 텐서는 배열로 합치면 4차원 텐서가 된다.
   4차원 텐서를 배열로 합치면 4차원 텐서가 된다.
   이런 식으로 텐서는 배열로서 계속해서 확장될 수 있다.

   ![img](https://wikidocs.net/images/page/37001/tensor.png)

   위의 그림은 각 텐서를 도형으로 시각화한 모습을 보여준다.

6. 케라스에서의 텐서
   앞서 Numpy로 각 텐서의 ndim(차원)과 shape(크기)를 출력했었다.
   예를 들어 위의 예제에서는 3차원 텐서는 3차원이고 크기는 (2, 3, 5)였다.
   케라스에서는 입력의 크기(shape)를 인자로 줄 때 input_shape라는 인자를 사용한다.

   자세한 사용 예제는 뒤에서 보겠지만 input_shape는 배치 크기를 제외하고 차원을 지정하는데, 예를 들어 input_shape(6, 5)라는 인자값을 사용하고 배치 크기를 32라고 지정한다면 이 텐서의 크기는 (32, 6, 5)을 의미한다.
   만역 배치 크기까지 지정해주고 싶다면 batch_input_shape=(8, 2, 10)와 같이 인자를 주면 이 텐서의 크기는 8, 2, 10)을 의미한다.

   그 외에도 입력의 속성 수를 의미하는 input_dim, 시퀀스 데이터의 길이를 의미하는 input_length 등의 인자도 사용한다.
   사실 input_shape의 두 개의 인자는 (input_length, input_dim)라고 볼 수 있다.

---

### 6.3 벡터와 행렬의 연산

여기서는 벡터와 행렬의 기본적인 연산에 대해서 알아보자.
아래의 모든 실습은 Numpy를 아래와 같이 import해야 한다.

```python
import numpy as np
```

1. 벡터와 행렬의 덤셈과 뺄셈
   같은 크기의 두 개의 벡터나 행렬은 덧셈과 뺄셈을 할 수 있다.
   이 경우 같은 위치의 원소끼리 연산하면 된다.
   이러한 연산을 요소별(element-wise) 연산이라고 한다.
   다음과 같이 a와 b라는 두 개의 벡터가 있다고 해보자.
   $$
   a = \left[
       \begin{array}{c}
         8 \\
         4 \\
         5 \\
       \end{array}
     \right]
   \ \ 
   b = \left[
       \begin{array}{c}
         1 \\
         2 \\
         3 \\
       \end{array}
     \right]
   $$
   이 때 두 벡터 a와 b의 덧셈과 뺄셈은 아래와 같다.
   $$
   a + b = \left[
       \begin{array}{c}
         8 \\
         4 \\
         5 \\
       \end{array}
     \right]
   + \left[
       \begin{array}{c}
         1 \\
         2 \\
         3 \\
       \end{array}
     \right]
   = \left[
       \begin{array}{c}
         9 \\
         6 \\
         8 \\
       \end{array}
     \right]
   $$

   $$
   a - b = \left[
       \begin{array}{c}
         8 \\
         4 \\
         5 \\
       \end{array}
     \right]
   - \left[
       \begin{array}{c}
         1 \\
         2 \\
         3 \\
       \end{array}
     \right]
   = \left[
       \begin{array}{c}
         7 \\
         2 \\
         2 \\
       \end{array}
     \right]
   $$

   Numpy를 이용하여 이를 구현할 수 있다.

   ```
   a = np.array([8, 4, 5])
   b = np.array([1, 2, 3])
   print(a+b)
   print(a-b)
   ```

   ```
   [9 6 8]
   [7 2 2]
   ```

   행렬도 마찬가지이다.
   다음과 가타이 a와 b라는 두 개의 행렬이 있다고 하였을 때, 두 행렬 a와 b의 덧셈과 뺄셈은 아래와 같다.
   $$
   a = 
   \left[
       \begin{array}{c}
         10\ 20\ 30\ 40\\
         50\ 60\ 70\ 80\\
       \end{array}
     \right]
   \ \ \ 
   b = 
   \left[
       \begin{array}{c}
         5\ 6\ 7\ 8\\
         1\ 2\ 3\ 4\\
       \end{array}
     \right]
   $$

   $$
   a + b = \left[
       \begin{array}{c}
         10\ 20\ 30\ 40\\
         50\ 60\ 70\ 80\\
       \end{array}
     \right] 
   + \left[
       \begin{array}{c}
         5\ 6\ 7\ 8\\
         1\ 2\ 3\ 4\\
       \end{array}
     \right]
   = \left[
       \begin{array}{c}
         15\ 26\ 37\ 48\\
         51\ 62\ 73\ 84\\
       \end{array}
     \right]
   $$

   $$
   a - b = \left[
       \begin{array}{c}
         10\ 20\ 30\ 40\\
         50\ 60\ 70\ 80\\
       \end{array}
     \right] 
   - \left[
       \begin{array}{c}
         5\ 6\ 7\ 8\\
         1\ 2\ 3\ 4\\
       \end{array}
     \right]
   = \left[
       \begin{array}{c}
         5\ 14\ 23\ 32\\
         49\ 58\ 67\ 76\\
       \end{array}
     \right]
   $$

   Numpy를 이용하여 이를 구현할 수 있다.

   ```
   import numpy as np
   a = np.array([[10, 20, 30, 40], [50, 60, 70, 80]])
   b = np.array([[5, 6, 7, 8],[1, 2, 3, 4]])
   print(a+b)
   print(a-b)
   ```

   ```
   [[15 26 37 48]
    [51 62 73 84]]
   [[ 5 14 23 32]
    [49 58 67 76]]
   ```

2. 벡터의 내적과 행렬의 곱셈
   벡터의 점곱(dot product) 또는 내적(inner product)에 대해 알아보자.
   벡터의 내적은 연산을 점(dot)으로 표현하여 a-b와 같이 표현하기도 한다.

   내적이 성립하기 위해서는 두 벡터의 차원이 같아야 하며, 두 벡터 중 앞의 벡터가 행벡터(가로 방향 벡터)이고 뒤의 벡터가 열벡터(세로 방향 벡터)여야 한다.
   아래는 두 벡터의 차원이 같고 곱셈의 대상이 각각 행벡터이고 열벡터일 때 내적이 이루어지는 모습을 보여준다.
   벡터의 내적의 결과는 스칼라가 된다는 특징이 있다.
   $$
   a \cdot b =
   \left[
       \begin{array}{c}
         1\ 2\ 3
       \end{array}
     \right]
   \left[
       \begin{array}{c}
         4 \\
         5 \\
         6 \\
       \end{array}
     \right]
   = 1 × 4 + 2 × 5 + 3 × 6 = 32\text{(스칼라)}
   $$
   Numpy를 이용하여 이를 구현할 수 있다.

   ```
   import numpy as np
   a = np.array([[1, 3],[2, 4]])
   b = np.array([[5, 7],[6, 8]])
   print(np.matmul(a,b))
   ```

   ```
   [[23 31]
    [34 46]]
   ```

   지금까지 벡터의 내적과 행렬의 곱셈에 대해서 알아보았는데, 벡터와 행렬의 곱 또는 행렬과 벡터의 곱 또한 행렬의 곱셈과 동일한 원리로 이루어진다.
   이에 대해서는 아래의 예제를 통해 이해할 수 있다.

---

### 6.3 다중 선형 회귀 행렬 연산으로 이해하기

독립 변수가 2개 이상일 때, 1개의 종속 변수를 예측하는 문제를 행렬의 연산으로 표현한다면 어떻게 될까?
다중 선형 회귀나 다중 로지스틱 회귀가 이러한 연산의 예인데, 여기서는 다중 선형 회귀를 통해 예를 들어보자.
다음은 독립 변수 x가 n개인 다음과 같은 다중 선형 회귀 수식이다.
$$
y = w_1x_1 + w_2x_2 + w_3x_3 + ... + w_nx_n + b
$$
이는 입력 벡터 [x1,...xn]와 가중치 벡터 [w1,...,wn]의 내적으로 표현할 수 있다.
$$
y = 
\left[
    \begin{array}{c}
      x_{1}\ x_{2}\ x_{3}\ \cdot\cdot\cdot\ x_{n}
    \end{array}
  \right]
\left[
    \begin{array}{c}
      w_{1} \\
      w_{2} \\
      w_{3} \\
      \cdot\cdot\cdot \\
      w_{n}
    \end{array}
  \right]
+
b
= x_1w_1 + x_2w_2 + x_3w_3 + ... + x_nw_n + b
$$
또는 가중치 벡터  [w1,...,wn]와 입력 벡터 [x1,...xn]의 내적으로 표현할 수도 있다.
$$
y = 
\left[
    \begin{array}{c}
      w_{1}\ w_{2}\ w_{3}\ \cdot\cdot\cdot\ w_{n}
    \end{array}
  \right]
\left[
    \begin{array}{c}
      x_{1} \\
      x_{2} \\
      x_{3} \\
      \cdot\cdot\cdot \\
      x_{n}
    \end{array}
  \right]
+
b
= x_1w_1 + x_2w_2 + x_3w_3 + ... + x_nw_n + b
$$
그런데 데이터의 개수가 많을 경우에는 벡터의 내적이 아니라 행렬의 곱셈으로 표현이 가능하다.
다중 선형 회귀에서 데이터의 개수가 여러 개일 때, 행렬의 곱셈으로 어떻게 표현할 수 있는지 예를 들어보자.
다음은 집의 크기, 방의 수 , 층의 수, 집이 얼마나 오래되었는지와 집의 가격이 기록된 부동산 데이터라고 가정하자.
해당 데이터를 학습하여 새로운 집의 정보가 들어왔을 때, 집의 가격을 예측해본다고 하자.

| size(feet^2)(x1) | number of bedrooms(x2) | number of floors(x3) | age of home(x4) | price($1000)(y) |
| :--------------: | :--------------------: | :------------------: | :-------------: | :-------------: |
|       1800       |           2            |          1           |       10        |       207       |
|       1200       |           4            |          2           |       20        |       176       |
|       1700       |           3            |          2           |       15        |       213       |
|       1500       |           5            |          1           |       10        |       234       |
|       1100       |           2            |          2           |       10        |       155       |

위의 데이터에 대해서 입력 행렬 X와 가중치 벡터 W의 곱으로 표현하면 다음과 같다.
$$
\left[
    \begin{array}{c}
      x_{11}\ x_{12}\ x_{13}\ x_{14} \\
      x_{21}\ x_{22}\ x_{23}\ x_{24} \\
      x_{31}\ x_{32}\ x_{33}\ x_{34} \\
      x_{41}\ x_{42}\ x_{43}\ x_{44} \\
      x_{51}\ x_{52}\ x_{53}\ x_{54} \\
    \end{array}
  \right]
\left[
    \begin{array}{c}
      w_{1} \\
      w_{2} \\
      w_{3} \\
      w_{4} \\
    \end{array}
  \right]
  =
\left[
    \begin{array}{c}
      x_{11}w_{1}+ x_{12}w_{2}+ x_{13}w_{3}+ x_{14}w_{4} \\
      x_{21}w_{1}+ x_{22}w_{2}+ x_{23}w_{3}+ x_{24}w_{4} \\
      x_{31}w_{1}+ x_{32}w_{2}+ x_{33}w_{3}+ x_{34}w_{4} \\
      x_{41}w_{1}+ x_{42}w_{2}+ x_{43}w_{3}+ x_{44}w_{4} \\
      x_{51}w_{1}+ x_{52}w_{2}+ x_{53}w_{3}+ x_{54}w_{4} \\
    \end{array}
  \right]
$$
여기에 편향 벡터 B를 더 해주면 위의 데이터에 대한 전체 가설 수식 H(X)를 표현할 수 있다.
$$
\left[
    \begin{array}{c}
      x_{11}w_{1}+ x_{12}w_{2}+ x_{13}w_{3}+ x_{14}w_{4} \\
      x_{21}w_{1}+ x_{22}w_{2}+ x_{23}w_{3}+ x_{24}w_{4} \\
      x_{31}w_{1}+ x_{32}w_{2}+ x_{33}w_{3}+ x_{34}w_{4} \\
      x_{41}w_{1}+ x_{42}w_{2}+ x_{43}w_{3}+ x_{44}w_{4} \\
      x_{51}w_{1}+ x_{52}w_{2}+ x_{53}w_{3}+ x_{54}w_{4} \\
    \end{array}
  \right]
+
\left[
    \begin{array}{c}
      b \\
      b \\
      b \\
      b \\
      b \\
    \end{array}
  \right]
= 
\left[
    \begin{array}{c}
      y_{1}\\ y_{2}\\ y_{3}\\ y_{4}\\ y_{5} \\
    \end{array}
  \right]
$$

$$
H(X) = XW + B
$$

위의 수식에서 입력 행렬 X는 5행 4열의 크기를 가진다.
출력 벡터를 Y라고 하였을 때 Y는 5행 1열의 크기를 가진다.
여기서 곱셈이 성립하기 위해서 가중치 벡터 W의 크기는 4행 1열을 가져야함을 추론할 수 있다.

만약 가중치 벡터를 앞에 두고 입력 행렬을 뒤에 두고 행렬 연산을 한다면 이는 아래와 같다.
$$
\left[
    \begin{array}{c}
      w_{1}\ w_{2}\ w_{3}\ w_{4} \\
    \end{array}
  \right]
\left[
    \begin{array}{c}
      x_{11}\ x_{21}\ x_{31}\ x_{41}\ x_{51}\\
      x_{12}\ x_{22}\ x_{32}\ x_{42}\ x_{52}\\
      x_{13}\ x_{23}\ x_{33}\ x_{43}\ x_{53}\\
      x_{14}\ x_{24}\ x_{34}\ x_{44}\ x_{54}\\
    \end{array}
  \right]
+
\left[
    \begin{array}{c}
      b\ b\ b\ b\ b \\
    \end{array}
  \right]
=
\left[
    \begin{array}{c}
      y_{1}\ y_{2}\ y_{3}\ y_{4}\ y_{5} \\
    \end{array}
  \right]
$$
이는 연산 방법의 차이일뿐 어떤 것이 맞다고, 틀리다고 할 수 없다.
다만, 수학적 관례로 아래와 같이 수식으로 표현할 때는 주로 가중치 W가 입력 X의 앞에 오므로 위 두 가지 경우를 모두 염두하는 것이 좋다.
$$
H(X) = WX + B
$$
인공 신경망도 본질적으로 위와 같이 훈련 데이터에 대한 행렬 연산이므로 이번 예제는 반드시 이해해야 한다.

---

### 6.4 샘플(Sample)과 특성(Feature)

훈련 데이터의 입력 행렬을 X라고 하였을 때 샘플(Sample)과 특성(Feature)의 정의는 다음과 같다.

![img](https://wikidocs.net/images/page/35821/n_x_m.PNG)

ML에서는 데이터를 셀 수 있는 단위로 구분할 때, 각각을 샘플이라고 부르며, 종속 변수 y를 예측하기 위한 각각의 독립 변수 x를 특성이라고 부른다.

---

### 6.5 가중치와 편향 행렬의 크기 결정

여기서는 특성을 행렬의 열로 보는 경우를 가정하여 행렬의 크기가 어떻게 결정되는지 정리한다.

행렬곱은 두 가지 정의를 가지는데, 두 개의 행렬 J와 K의 곱은 다음과 같은 조건을 충족해야 한다.

1. 두 행렬의 곱 J x K에 대하여 행렬 J의 열의 수와 행렬 K의 행의 수는 같아야 한다.
2. 두 행렬의 곱 J x K의 결과로 나온 행렬 JK의 크기는 J의 행의 크기와 K의 열의 크기르 가진다.

이로부터 주어진 데이터가 입력과 출력의 행렬의 크기를 어떻게 가지느냐에 따라서 가중치 W의 행렬과 편향 b의 행렬의 크기를 찾아낼 수 있다.
독립 변수 x의 행렬을 X, 종속 변수 y의 행렬을 Y라고 했을 때, 가중치 W의 행렬을 W, 편향 b의 행렬을 B라고 해보자.
이때 행렬 X는 또한 입력 행렬(Input Matrix)이라고 부를 수 있고, Y는 출력 행렬(Output Matrix)이라고 부를 수 있다.

![img](https://wikidocs.net/images/page/37001/matrix1.PNG)

이제 입력 행렬릐 크기와 출력 행렬의 크기로부터 W행렬과 B행렬의 크기를 추론해보자.

![img](https://wikidocs.net/images/page/37001/matrix2.PNG)

우선 행렬의 덧셈에 해당되는 B행렬은 Y행렬의 크기에 영향을 주지 않는다.
그러므로 B행렬의 크기는 Y행렬의 크기와 같다.

![img](https://wikidocs.net/images/page/37001/matrix3.PNG)

행렬의 곱셈이 성립되려면 행렬의 곱셈에서 앞에 있는 행렬의 열의 크기와 뒤에 있는 행렬의 행의 크기는 같아야 한다.
그러므로 입력 행렬 X로부터 W행렬의 행의 크기가 결정된다.

![img](https://wikidocs.net/images/page/37001/matrix4.PNG)

두 행렬의 곱의 결과로서 나온 행렬의 열의 크기는 행렬의 곱에서 뒤에 있는 행렬의 열의 크기와 동일하다.
그러므로 출력 행렬 Y로부터 W행렬의 열의 크기가 결정된다.
배치에 대한 개념을 정리해보자.
이 때 위의 식에서 X행렬을 행을 의미하는 수치 m은 샘플 데이터를 몇 개씩 묶어서 처리하느냐에 따라 달라진다.
전체 샘플 데이터에 대해서 한 꺼번에 행렬 연산을 하고자 한다면, m은 전체 샘플의 개수가 된다.
위에서 본 다중 선형 회귀에서 5개의 전체 샘플을 가지고 세웠던 행렬 연산식이 그러한 예이다.

하지만 전체 샘플 데이터 중 1개씩 불러와서 처리하고자 한다면 m은 1이된다.
또는 전체 데이터를 임의의 m개씩 묶인 작은 그룹들로 분할하여 여러번 처리할 수도 있는데 이렇게 처리하면서 기계가 학습하는 것을 미니배치 학습이라고 한다.
예를 들어 전체 데이터가 1,024개 있을 때 m을 64로 샘플로 구성된다.
그리고 위에서 설명한 행렬 연산을 총 16번 반복하게 되고 그제서야 전체 데이터에 대한 학습이 완료된다.
이때 64를 배치 크기(Batch size)라고 한다.

---

### 7. 소프트맥스 회귀(Softmax Regression) - 다중 클래스 분류

로지스틱 회귀를 통해 2개의 선택지 중에서 1개를 고르는 이진 분류(Binary Classification)를 풀어봤다.
이번에는 3개 이상의 선택지 중에서 1개를 고르는 다중 클래스 분류 문제를 위한 소프트맥스 회귀(Softmax Regression)에 대해서 배우자.

---

### 7.1 다중 클래스 분류(Multi-class Classification)

이진 분류가 두 개의 선택지 중 하나를 고르는 문제였다면, 세 개 이상의 선택지 중 하나를 고르는 문제를 다중 클래스 분류라고 한다.
아래의 문제는 꽃받침 길이, 꽃받침 넓이, 꽃잎 길이, 꽃잎 넓이로부터 setosa, versicolor, virginica라는 3개의 품종 중 어떤 품종인지를 예측하는 문제로 전형적인 다중 클래스 분류 문제이다.

| SepalLengthCm(x1) | SepalWidthCm(x2) | PetalLengthCm(x3) | PetalWidthCm(x4) | Species(y) |
| :---------------: | :--------------: | :---------------: | :--------------: | :--------: |
|        5.1        |       3.5        |        1.4        |       0.2        |   setosa   |
|        4.9        |       3.0        |        1.4        |       0.2        |   setosa   |
|        5.8        |       2.6        |        4.0        |       1.2        | versicolor |
|        6.7        |       3.0        |        5.2        |       2.3        | versicolor |
|        5.6        |       2.8        |        4.9        |       2.0        | virginica  |

앞서 로지스틱 회귀에서 사용한 시그모이드 함수는 입력된 데이터에에 대해서 0과 1사이이의 값을 출력하여 해당 값이 둘 중 하나에 속할 확률로 해석할 수 있도록 만들어주었다.
예를 들어 0이 정상 메일, 1이 스팸 메일이라고 정의해놓는다면 시그모이드 함수의 0과 1사이의 출력값을 스팸 메일일 확률로 판단할 수 있었다.
확률값이 0.5를 넘으면 1에 더 가까우니까 스팸 메일로 판단라면 되고, 그 반대라면 정상 메일로 판단하면 되었다.

그런데 이번에는 세 개 이상의 정답지 중에서 고르는 문제이다.
여기다가 앞서 배운 시그모이드 함수를 사용해본다고 하자.
어쩌면 입력된 샘플 데이터에 대해서 각 정답지에 대해서 시그모이드 함수를 적용해볼 수 있다.
만약 그렇게 한다면 첫번째가 정답인 확률은 0.7, 두번째가 정답인 확률은 0.6, 세번째 정답일 확률은 0.4 등과 같은 출력을 얻게된다.
그런데 이 전체 확률의 합계가 1이 되도록 하여 전체 정답지에 걸친 확률로 바꿀 순 없을까.

만약 하나의 샘플 데이터에 대한 예측값으로 모든 가능한 정답지에 대한 정답일 확률의 합이 1이 되도록 구할 수 있다면 어떨까.
위의 예제에서 setosa, versicolor, virginica 3개 중 하나의 정답을 골라야 하는데 이 샘플 데이터가 setosa일 확률이 0.58, versicolor일 확률 0.22, virginica일 확률이 0.2와 같이 총 합이 1인 확률 분포를 구할 수 있게 해보자 것이다.
이럴 때 사용할 수 잇는 것이 소프트맥스 함수이다.

---

### 7.2 소프트맥스 함수(Softmax function)

소프트맥스 함수는 분류해야하는 정답(클래스)의 총 개수를 k라고 할 때, k차원의 벡터를 입력받아 각 클래스에 대한 확률을 추청한다.
우선 수식에 대해 설명하고, 그 후에는 그림으로 이해해보자.

1. 소프트맥스 함수의 이해
   k차원의 벡터에서 i번째 원소를 zi, i번째 클래스가 정답일 확률을 pi로 나타낸다고 하였을 때 소프트맥스 함수는 pi를 다음과 같이 정의한다.
   $$
   p_{i}=\frac{e^{z_{i}}}{\sum_{j=1}^{k} e^{z_{j}}}\ \ for\ i=1, 2, ... k
   $$
   위에서 풀어야하는 문제에 소프트맥스 함수를 차근차근 적용해보자.
   위에서 풀어야하는 문제의 경우 k=3이므로 3차원 벡터 z=[z1 z2 z3]의 입력을 받으면 소프트맥스 함수는 아래와 같은 출력을 리턴한다.
   $$
   softmax(z)=[\frac{e^{z_{1}}}{\sum_{j=1}^{3} e^{z_{j}}}\ \frac{e^{z_{2}}}{\sum_{j=1}^{3} e^{z_{j}}}\ \frac{e^{z_{3}}}{\sum_{j=1}^{3} e^{z_{j}}}] = [p_{1}, p_{2}, p_{3}] = \hat{y} = \text{예측값}
   $$
   p1, p2, p3 각각은 1번 클래스가 정답일 확률, 2번 클래스가 정답인 확률, 3번 클래스가 정답인 확률을 나타내며 각각 0과 1사이의 값으로 총 합은 1이 된다.
   여기서 분류하고자하는 3개의 클래스는 virginica, setosa, versicolor이므로 이는 결국 주어잔 입력이 virginica일 확률, setosa일 확률, versicolor이므로 이는 결국 주어진 입력이 virginica일 확률, setosa일 확률, versicolor일 확률을 나타내는 값을 의미한다.
   여기서는 i가 1일 때 virginica일 확률을 나타내고, 2일 때는 setosa일 확률, 3일 때는 versicolor일 확률이라고 지정하였다고 하자.
   이 지정 순서는 문제를 풀고자하는 사람의 무작위 선택이다.
   이에 따라 식을 문제에 맞게 다시 쓰면 아래와 같다.
   $$
   softmax(z)=[\frac{e^{z_{1}}}{\sum_{j=1}^{3} e^{z_{j}}}\ \frac{e^{z_{2}}}{\sum_{j=1}^{3} e^{z_{j}}}\ \frac{e^{z_{3}}}{\sum_{j=1}^{3} e^{z_{j}}}] = [p_{1}, p_{2}, p_{3}] = [p_{virginica}, p_{setosa}, p_{versicolor}]
   $$
   다소 복잡해보이지만 어려운 개념이 아니다.
   분류하고자 하는 클래스가 k개일 때, k차원의 벡터를 입력 받아서 모든 벡터 원소의 값을 0과 1사이의 값으로 값을 변경하여 다시 k차원의 벡터를 리턴한다는 내용을 식으로 기재하였을 뿐이다.
   방금 배운 개념을 그림을 통해 다시 설명하면서 더 깊이 들어가보자.

2. 그림을 통한 이해

   ![img](https://wikidocs.net/images/page/35476/softmax1_final_final.PNG)

   위의 그림에 점차 살을 붙여보자.
   여기서는 샘플 데이터를 1개씩 입력으로 받아 처리한다고 가정해보자.
   즉, 배치 크기는 1이다.

   위의 그림에는 두 가지 질문이 있다.
   첫번째 질문은 소프트맥스 함수의 입력에 대한 질문이다.
   하나의 샘플 데이터는 4개의 독립 변수 x를 가지는데 이는 모델이 4차원 벡터를 입력으로 받음을 의미한다.
   그런데 소프트맥스의 함수의 입력으로 사용되는 벡터는 벡터의 차원이 분류하고자 하는 클래스의 개수가 되어야 하므로 어떤 가중치 연산을 통해 3차원 벡터로 변환되어야 한다.
   위의 그림에서는 소프트맥스 함수의 입력으로 사용되는 3차원 벡터를 z로 표현하였다.

   ![img](https://wikidocs.net/images/page/35476/softmaxbetween1and2.PNG)

   샘플 데이터 벡터를 소프트맥스 함수의 입력 벡터로 축소하는 방법은 간단한다.
   소프트맥스 함수의 입력 벡터 z의 차원수만큼 결과값이 나오도록 가중치 곱을 진행한다.
   위의 그림에서 화살표는 총 (4 * 3 =12) 12개이며 전부 다른 가중치를 가지고, 학습 과정에서 점차적으로 오차를 최소화하는 가중치로 값이 변경된다.

   두번째 질문은 오차 계싼 방법에 대한 질문이다.
   소프트맥스 함수의 출력은 분류하고자하는 클래스의 개수만큼 차원을 가지는 벡터로 각 원소는 0과 1사이의 값을 가진다.
   이 각각은 특정 클래스가 정답일 확률을 나타낸다.
   여기서는 첫번째 원소인 p1은 virginica가 정답일 확률, 두번째 원소인 p2는 setosa가 정답일 확률, 세번째 원소인 p3는 versicolor가 정답일 확률로 고려하고자 한다.
   그렇다면 이 예측값과 비교를 할 수 있는 실제값의 표현 방법이 있어야 한다.
   소프트맥스 회귀에서 실제값을 원-핫 벡터로 표현한다.

   ![img](https://wikidocs.net/images/page/35476/softmax2_final.PNG)

   위의 그림은 소프트맥스 함수의 출력 벡터의 첫번째 원소 p1인 virginica가 정답일 확룰, 두번째 원소 p2가 setosa가 정답일 확률, 세번째 원소 p3가 versicolor가 정답일 확률을 의미한다고 하였을 때, 각 실제값의 정수 인코딩은 1, 2, 3이 되고 이에 원-핫 인코딩을 수행하여 원-핫 벡터로 수치화한 것을 보여준다.

   ![img](https://wikidocs.net/images/page/35476/softmax4.PNG)

   예를 들어 현재 풀고 있는 샘플 데이터의 실제값이 setosa라면 setosa의 원-핫 벡터는 [0 1 0]이다.
   이 경우, 예측값과 실제값의 오차가 0이 되는 경우는 소프트맥스 함수의 결과가 [0 1 0]이 되는 경우이다.
   이 두 벡터의 오차를 계산하기 위해서 소프트맥스 회귀는 비용 함수로 크로스 엔트로피 함수를 사용한다.

   ![img](https://wikidocs.net/images/page/35476/softmax5.PNG)

   이제 앞서 배운 선형 회귀나 로지스틱 회귀와 마찬가지로 오차로부터 가중치를 업데이트 한다.

   ![img](https://wikidocs.net/images/page/35476/softmax6_final.PNG)

   더 정확히는 선형 회귀나 로지스틱 회귀와 마찬가지로 편향 또한 업데이트의 대상이 되는 매개 변수이다.
   소프트맥스 회귀를 벡터와 행렬 연산으로 이해해보자.
   입력을 특성(feature)의 수만큼의 차원을 가진 입력 벡터 x라고 하고, 가중치 행렬을 W, 편향을 b라고 하였을 대, 소프트맥스 회귀에서 예측값을 구하는 과정을 벡터와 행렬 연산이라고 표현하면 아래와 같다.

   ![img](https://wikidocs.net/images/page/35476/softmax7.PNG)

   여기서는 4는 특성의 수 이며 3은 클래스의 개수에 해당된다.

---

### 7.3 원-핫 벡터의 무작위성

꼭 실제값을 원-핫 벡터로 표현해야만 다중 클래스 분류 문제를 풀 수 있는 것은 아니지만, 대부분의 다중 클래스 분류 문제가 각 클래스 간의 관계가 균등하다는 점에서 원-핫 벡터는 이러한 점을 표현할 수 있는 적절한 방법이다.

다수의 클래스를 분류하는 문제에서는 이진 분류처럼 2개의 숫자 레이블이 아니라 클래스의 개수만큼 숫자 레이블이 필요하다.
이때 직관적으로 생각해볼 수 있는 레이블링 방법은 분류해야 할 클래스 전체에 정수 인코딩을 하는 것이다.
예를 들어서 분류해야 할 레이블이 {red, green, blue}와 같이 3개라면 각각 0, 1, 2로 레이블을 합친다.
또는 분류해야 할 클래스가 4개고 인덱스를 숫자 1부터 시작하고 싶다면  하면 {baby, child, adolescent, adult}라면 1, 2, 3, 4로 레이블을 해볼 수 있다.
그런데 일반적인 다중 클래스 분류 문제에서 레이블링 방법으로는 위와 같은 정수 인코딩이 아니라 원-핫 인코딩을 사용하는 것이 보다 클래스의 성질을 잘 표현하였다고 할 수 있다.
그 이유를 알아보자.

Banana, Tomato, Apple이라는 3개의 클래스가 존재하는 문제가 있다고 해보자.
레이블은 정수 인코딩을 사용하여 각각 1, 2, 3을 부여했다.
손실 함수로 선형 회귀에서 배운 평균 제곱 오차(MSE)를 사용하면 정수 인코딩이 어떤 오해를 불러일으킬 수 있는지 확인 할 수 있다.
아래의 식은 앞서 선형 회귀에서 배웠던 MSE를 다시 그대로 가져온 것이다.
y^은 예측값을 의미한다.
$$
Loss\ function = \frac{1}{n} \sum_i^{n} \left(y_{i} - \hat{y_{i}}\right)^2
$$
직관적인 오차 크기 비교를 위해 평균을 구하는 수식은 제외하고 제곱 오차로만 판단해보자.

실제값이 Tomato일때 예측값이 Banana이었다면 제곱 오차는 다음과 같다.
$$
(2-1)^{2} = 1
$$
실제값이 Apple일때 예측값이 Banana이었다면 제곱 오차는 다음과 같다.
$$
(3-1)^{2} = 4
$$
즉, Banana와 Tomato 사이의 오차보다 Banana와 Apple의 오차가 더 크다.
이는 기계에게 Banana가 Apple보다는 Tomato에 더 가깝다는 정보를 주는 것과 다름없다.
더 많은 클래스에 대해서 정수 인코딩을 수행했다고 해보자.

- {Banana :1, Tomato :2, Apple :3, Strawberry :4, ... Watermelon :10}

이 정수 인코딩은 Banana가 Watermelon보다는 Tomato에 더 가깝다는 의미를 담고 있다.
이는 사용자가 부여하고자 했던 정보가 아니다.
이러한 정수 인코딩의 순서 정보가 도움이 되는 분류 문제도 물론 존재한다.
바로 각 클래스가 순서의 의미를 갖고 있어서 회귀를 통해서 분류 문제를 풀 수 있는 경우이다.
예를 들어 {baby, child, adolescent, adult}나 1층, 2층, 3층, 4층}이나 {10대, 20대, 30대, 40대}와 같은 경우가 이에 해당된다.
하지만 일반적인 분류 문제에서는 각 클래스는 순서의 의미를 갖고 있지 않으므로 각 클래스 간의 오차는 균등한 것이 옳다.
정수 인코딩과 달리 원-핫 인코딩은 분류 문제 클래스 간의 관계를 균등하게 분배한다.

아래는 세 개의 카테고리에 대해서 원-핫 인코딩을 통해서 레이블을 인코딩했을 때 각 클래스 간의 제곱 오차가 균등함을 보여준다.
$$
((1,0,0)-(0,1,0))^{2} = (1-0)^{2} + (0-1)^{2} + (0-0)^{2} = 2
$$

$$
((1,0,0)-(0,0,1))^{2} = (1-0)^{2} + (0-0)^{2} + (0-1)^{2} = 2
$$

다르게 표현하면 모든 클래스에 대해서 원-핫 인코딩을 통해 얻은 원-핫 벡터들은 모든 쌍에 대해서 유클리드 거리를 구해도 전부 유클리드 거리가 동일하다.
원-핫 벡터는 이처럼 각 클래스의 표현 방법이 무작위성을 가진다는 점을 표현할 수 있다.
뒤에서 다시 언급되겠지만 이러한 원-핫 벡터의 관계의 무작위성은 때로는 단어의 유사성을 구할 수 없다는 단점으로 언급되기도 한다.

---

### 7.4 비용 함수(Cost function)

소프트맥스 회귀에서는 비용 함수로 크로스 엔트로피 함수를 사용한다.
여기서는 소프트맥스 회귀에서의 크로스 엔트로피 함수뿐만 아니라, 다양한 표기 방법에 대해서 이해해보자.

1. 크로스 엔트로피 함수
   아래에서 y는 실제값을 나타내며, k는 클래스의 개수로 정의한다.
   yj는 실제값 원-핫 벡터의 j번째 인덱스를 의미하며, pj는 샘플 데이터가 j번째 클래스일 확률을 나타낸다.
   표기에 따라서 y^j로 표현하기도 한다.
   $$
   cost(W) = -\sum_{j=1}^{k}y_{j}\ log(p_{j})
   $$
   이 함수가 왜 비용 함수로 적합한지 알아보자.
   c가 실제값 원-핫 벡터에서 1을 가진 원소의 인덱스라고 한다면, pc=1은 y^가 y를 정확하게 예측한 경우가 된다.
   이를 식에 대입해보면 -1log(1)=0이 되기 때문에, 결과적으로 y^가 y를 정확하게 예측한 경우의 크로스 엔트로피 함수의 값은 0이 된다.
   즉, 
   $$
   -\sum_{j=1}^{k}y_{j}\ log(p_{j})
   $$
   이 값을 최소화하는 방향으로 학습해야 한다.

   이제 이를 n개의 전체 데이터에 대한 평균을 구한다고 하면 최종 비용 함수는 다음과 같다.
   $$
   cost(W) = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{k}y_{j}^{(i)}\ log(p_{j}^{(i)})
   $$

2. 이진 분류에서의 크로스 엔트로피 함수
   로지스틱 회귀에서 배운 크로스 엔트로피 함수식과 달라보이지만, 본질적으로 동일한 함수식이다.
   로지스틱 회귀의 크로스 엔트로피 함수식으ㅡ로부터 소프트맥스 회귀의 크로스 엔트로피 함수식을 도출해보자.
   $$
   cost(W) = -(y\ logH(X) + (1-y)\ log(1-H(X)))
   $$
   위의 식은 앞서 로지스틱 회귀에서 배웠던 크로스 엔트로피의 함수식을 보여준다.
   위의 식에서 y를 y1, y-1을 y2로 치환하고 H(X)를 p1, 1-H(X)를 p2로 치환해보자.
   결과적으로 아래의 식을 얻을 수 있다.
   $$
   -(y_{1}\ log(p_{1})+y_{2}\ log(p_{2}))
   $$
   이 식은 아래와 같이 표현할 수 있다.
   $$
   -(\sum_{i=1}^{2}y_{i}\ log\ p_{i})
   $$
   소프트맥스 회귀에서는 k의 값이 고정된 값이 아니므로 2를 k로 변경한다.
   $$
   -(\sum_{i=1}^{k}y_{i}\ log\ p_{i})
   $$
   위의 식은 결과적으로 소프트맥스 회귀의 식과 동일하다.
   역으로 소프트맥스 회귀에서 로지스틱 회귀의 크로스 엔트로피 함수식을 얻는 것은 k를 2로 하고, y1과 y2를 각각 y와  1-y로 치환하고, p1과 p2를 각각 H(X)와 1-H(X)로 치환하면 된다.

   정리하면 소프트맥스 함수의 최종 비용 함수에서 k가 2라고 가정하면 결국 로지스틱 회귀의 비용 함수와 같다.
   $$
   cost(W) = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{k}y_{j}^{(i)}\ log(p_{j}^{(i)}) = -\frac{1}{n} \sum_{i=1}^{n} [y^{(i)}log(p^{(i)}) + (1-y^{(i)})log(1-p^{(i)})]
   $$

---

### 7.5 소프트맥스 회귀(Softmax Regression)

이번에 실습할 데이터는 앞서 다중 클래스 분류를 설명하기 위해 예를 들었던 품종 분류 문제이다.
데이터를 직접 다운로드 받고, 데이터에 대한 탐색 결과을 거친 후에 모델을 설계해보자.

1. 아이리스 품종 데이터에 대한 이해

   ```python
   import pandas as pd
   data = pd.read_csv('iris.csv 파일의 경로',encoding='latin1')
   # 예를 들어윈도우 바탕화면에서 작업한 저자의 경우에는
   # data = pd.read_csv(r'C:\Users\USER\Desktop\iris.csv',encoding='latin1')
   ```

   우선 iris.csv 파일을 불러온다.
   해당 데이터가 어떤 구성인지 확인하기 위해 5개의 샘플만 출력해보자.

   ```python
   print(len(data)) # 총 샘플의 개수 출력
   print(data[:5]) # 샘플 중 5개 출력
   ```

   ```python
   150
      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species
   0   1            5.1           3.5            1.4           0.2  Iris-setosa
   1   2            4.9           3.0            1.4           0.2  Iris-setosa
   2   3            4.7           3.2            1.3           0.2  Iris-setosa
   3   4            4.6           3.1            1.5           0.2  Iris-setosa
   4   5            5.0           3.6            1.4           0.2  Iris-setosa
   ```

   데이터는 총 150개의 샘플로 구성되어져 있으며 6개의 열로 구성되어져 있다.
   각 샘플의 인덱스를 의미하는 첫번째 열인 Id와 특성(feature)에 해당하는 SepalLengthCm, SepalWidthCm, PetalLengthCM, PetalWidthCm 4개의 열이 존재한다.
   그리고 마지막 열 Species는 해당 샘플이 어떤 품종인지를 의미한다.
   Species 열은 몇 가지 품종으로 구성되어져 있는지 출력해보자.

   ```python
   print("품종 종류:", data["Species"].unique(), sep="\n")
   # 중복을 허용하지 않고, 있는 데이터의 모든 종류를 출력
   ```

   ```python
   품종 종류:
   ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']
   ```

   Species는 Iris-setosa, Iris-versicolor, Iris-virginica라는 3개의 품종으로 구성되어져 있다.
   즉, 이번에는 데이터를 가지고 푸는 문제는 주어진 샘플 데이터의 4개의 특성으로부터 3개 중 어떤 품종인지를 예측하는 문제가 되겠다.
   3개의 품종이 4개의 특성에 대해서 어떤 분포를 가지고 있는지 시각화해보자.

   ```python
   import seaborn as sns
   #del data['Id'] # 인덱스 열 삭제
   sns.set(style="ticks", color_codes=True)
   g = sns.pairplot(data, hue="Species", palette="husl")
   ```

   ![img](https://wikidocs.net/images/page/35476/species.png)

   pairplot은 데이터프레임을 인수로 받아 데이터프레임의 각 열의 조합에 따라서 산점도(scatter plot)을 그린다.
   쉽게 말해 4개의 특성에 해당하는 SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm에 대해서 모든 쌍(pair)의 조합인 16개의 경우에 대해서 산점도를 그린다.

   만약 같은 데이터의 쌍일 경우에는 히스토그램으로 나타내는데, SepalLengthCm와 SepalLengthCm의 조합이나 SepalWidthCm과 SepalWidthCm의 조합일 경우가 히스토그램으로 나타낸다.

   ```python
   sns.barplot(data['Species'], data['SepalWidthCm'], ci=None)
   # 각 종과 특성에 대한 연관 관계
   ```

   ![img](https://wikidocs.net/images/page/35476/%EC%97%B0%EA%B4%80%EA%B4%80%EA%B3%84.png)

   barplot을 통해 종과 특성에 대한 연관관계를 출력할 수도 있다.
   150개의 샘플 데이터 중에서 각 품종인 Iris-setosa, Iris-versicolor, Iris-virginica을 나타내는 데이터가 몇 개씩 있는지 확인해보자.
   즉, Species열에서 각 품종이 몇 개있는지 확인한다.

   ```python
   import matplotlib.pyplot as plt
   data['Species'].value_counts().plot(kind='bar')
   ```

   ![img](https://wikidocs.net/images/page/35476/%ED%92%88%EC%A2%85%EA%B0%9C%EC%88%98.png)

   정확하게 50개씩 있는 것을 확인할 수 있다.
   즉, 각 품종 데이터에 대한 분포는 1:1:1이다.
   데이터에 대한 구성을 파악하였다면, 이제 소프트맥스 회귀 모델을 구성하기 위해 전처리를 진행해야 한다.
   우선 Species열에 대해서 전부 수치화를 진행할 필요가 있다.
   우선 원-핫 인코딩을 수행하기 전에 정수 인코딩을 수행하자.
   그리고 정상적으로 정수 인코딩이 수행되었는지 확인하기 위하여 다시 한 번 값의 분포를 출력하자.

   ```python
   data['Species'] = data['Species'].replace(['Iris-virginica','Iris-setosa','Iris-versicolor'],[0,1,2])
   # Iris-virginica는 0, Iris-setosa는 1, Iris-versicolor는 2가 됨.
   data['Species'].value_counts().plot(kind='bar')
   ```

   ![img](https://wikidocs.net/images/page/35476/%EA%B0%92%EC%9D%98%EB%B6%84%ED%8F%AC2.png)

   여전히 50개씩 1:1:1의 분포를 보인다.
   이제 특성과 품종을 각각 종속 변수와 독립 변수 데이터로 분리하는 작업을 수행하고, 정확하게 분리가 되었는지 확인하기 위해 데이터 중 5개씩만 출력해보자.

   ```python
   from sklearn.model_selection import train_test_split
   data_X = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].values # X 데이터에 해당됩니다. X는 총 4개입니다.
   data_y = data['Species'].values # Y 데이터에 해당됩니다. 예측해야하는 값입니다.
   
   print(data_X[:5]) #X에 해당되는 데이터를 10개만 출력합니다.
   print(data_y[:5]) #y에 해당되는 데이터를 10개만 출력합니다.
   ```

   ```python
   [[5.1 3.5 1.4 0.2]
    [4.9 3.  1.4 0.2]
    [4.7 3.2 1.3 0.2]
    [4.6 3.1 1.5 0.2]
    [5.  3.6 1.4 0.2]]
   [1 1 1 1 1]
   ```

   이제 훈련 데이터와 테스트 데이터의 분리와 원-핫 인코딩을 수행해보자.

   ```python
   (X_train, X_test, y_train, y_test) = train_test_split(data_X, data_y, train_size=0.8, random_state=1)
   # 훈련 데이터와 테스트 데이터를 8:2로 나눕니다. 또한 데이터의 순서를 섞습니다.
   from tensorflow.keras.utils import to_categorical
   y_train = to_categorical(y_train)
   y_test = to_categorical(y_test)
   # 훈련 데이터와 테스트 데이터에 대해서 원-핫 인코딩
   print(y_train[:5])
   print(y_test[:5])
   ```

   ```python
   [[0. 0. 1.]
    [1. 0. 0.]
    [0. 0. 1.]
    [1. 0. 0.]
    [1. 0. 0.]]
   [[0. 1. 0.]
    [0. 0. 1.]
    [0. 0. 1.]
    [0. 1. 0.]
    [1. 0. 0.]]
   ```

   이제 전처리 단계가 모두 끝이 났다.

2. 소프트맥스 회귀

   ```python
   from tensorflow.keras.models import Sequential # 케라스의 Sequential()을 임포트
   from tensorflow.keras.layers import Dense # 케라스의 Dense()를 임포트
   from tensorflow.keras import optimizers # 케라스의 옵티마이저를 임포트
   
   model=Sequential()
   model.add(Dense(3, input_dim=4, activation='softmax'))
   sgd=optimizers.SGD(lr=0.01)
   # 학습률(learning rate, lr)은 0.01로 합니다.
   model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])
   # 옵티마이저는 경사하강법의 일종인 adam을 사용합니다.
   # 손실 함수(Loss function)은 평균제곱오차 크로스 엔트로피 함수를 사용합니다.
   history=model.fit(X_train,y_train, batch_size=1, epochs=200, validation_data=(X_test, y_test))
   # 주어진 X와 y데이터에 대해서 오차를 최소화하는 작업을 200번 시도합니다.
   ```

   앞서 실습한 코드들과 거의 동일한데 달라진 점은 입력의 차원이 4로 바뀌면서, input_dim의 인자값이 4로 바뀌었다는 점과 이제 출력의 차원이 3이므로 input_dim=4 앞의 인자값이 3으로 바뀌었다는 점이다.
   또한 함수로는 소프트맥스 함수를 사용하므로 activation에는 softmax를 기재해준다.

   오차 함수로는 크로스 엔트로피 함수를 사용한다.
   이진 분류 문제에서는 binary_crossentropy를 사용하였지만, 다중 클래스 분류 문제에서는 'categorical_crossentropy를 기재해줘야 한다.
   옵티마이저로는 경사 하강법의 일종인 adam을 사용한다.
   전체 데이터에 대한 훈련 횟수는 200회로 주었다.
   이번에는 테스트 데이터를 별도로 분리해서 평가에 사용하였는데, validation_data=()에 테스트 데이터를 기재해주면 실제로는 훈련에는 반영되지 않으면서 각 훈련 횟수마다 테스트 데이터에 대한 정확도를 출력한다.
   즉, 정확도가 전체 데이터에 대한 훈련 1회(1 에포크)마다 측정되고는 있지만 기계는 저 데이터를 가지고는 가중치를 업데이트하지 않는다.

   ```python
   Train on 120 samples, validate on 30 samples
   Epoch 1/200
   120/120 [==============================] - 0s 4ms/step - loss: 2.3404 - acc: 0.3083 - val_loss: 1.7910 - val_acc: 0.4333
   ... 중략 ...
   Epoch 200/200
   120/120 [==============================] - 0s 852us/step - loss: 0.1711 - acc: 0.9500 - val_loss: 0.1767 - val_acc: 1.0000
   ```

   acc은 훈련 데이터에 대한 정확도이고, val_acc은 테스트 데이터에 대한 정확도를 의미한다.
   훈련 데이터에서는 95%의 정확도를 보이고, 테스트 데이터에 대해서는 100%의 정확도를 보인다.
   번에는 각 에포크당 훈련 데이터와 테스트 데이터에 대한 정확도를 측정했으므로 한 번 에포크에 따른 정확도를 그래프로 출력해보자.

   ```python
   epochs = range(1, len(history.history['accuracy']) + 1)
   plt.plot(epochs, history.history['loss'])
   plt.plot(epochs, history.history['val_loss'])
   plt.title('model loss')
   plt.ylabel('loss')
   plt.xlabel('epoch')
   plt.legend(['train', 'val'], loc='upper left')
   plt.show()
   ```

   ![img](https://wikidocs.net/images/page/35476/%ED%9B%88%EB%A0%A8_%EB%8D%B0%EC%9D%B4%ED%84%B0_%ED%85%8C%EC%8A%A4%ED%8A%B8_%EB%8D%B0%EC%9D%B4%ED%84%B0.png)

   에포크가 증가함에 따라 정확도가 점차적으로 줄어드는 것을 볼 수 있다.
   이미 테스트 데이터에 대한 정확도를 validation_data=()를 통해 알고는 있지만 케라스에서 테스트 데이터의 정확도를 측정하는 용도로 제공하고 있는 evaluate()를 통해 테스트 데이터에 대한 정확도를 다시 출력해보자.

   ```python
   print("\n 테스트 정확도: %.4f" % (model.evaluate(X_test, y_test)[1]))
   ```

   ```python
   30/30 [==============================] - 0s 33us/step
   테스트 정확도: 1.0000
   ```

   테스트 데이터에 대해서 100%의 정확도를 얻었다.

---

### 7.6 인공 신경망 다이어그램

n개의 특성을 가지고 m개의 클래스를 분류하는 소프트맥스 회귀를 뒤에서 배우게 되는 인공 신경망의 형태로 표현하면 다음과 같다.
소프트맥스 회귀 또한 하나의 인공 신경망으로 볼 수 있으므로 소프트맥스 회귀로부터 인공 신경망을 이해하는 것은 어렵지 않다.

![img](https://wikidocs.net/images/page/35476/softmax_regression_nn.PNG)

사실 위의 그림은 앞서 소프트맥스 함수를 사용하기 위해 설명했던 아래의 그림에서 특성의 개수를 n으로 하고, 클래스의 개수를 m으로 일반화한 뒤에 그림을 좀 더 요약해서 표현한 것으로 봐도 무방하다.

![img](https://wikidocs.net/images/page/35476/softmax6_final.PNG)