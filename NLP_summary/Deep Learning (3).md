# Deep Learning (3)

Source: https://wikidocs.net/22882, https://wikidocs.net/24958, https://wikidocs.net/24987, https://wikidocs.net/36033, https://wikidocs.net/37406, https://wikidocs.net/61374, https://wikidocs.net/61375, https://wikidocs.net/32105, https://wikidocs.net/38861, https://wikidocs.net/49071, https://wikidocs.net/45609

---

### 8. 피드 포워드 신경망 언어 모델(Neural Network Language Model, NNLM)

파이썬 등과 같은 프로그래밍 언어를 사용할 때는 명세되어져 있는 튜플, 클래스 등과 같은 용어와 작성할 때 지켜야 하는 문법을 바탕으로 코드를 작성한다.
문법에 맞지 않으면 에러가 발생하므로 명세된 규칙을 지키는 것은 필수적이다.

자연어는 어떨까.
자연어에도 문법이라는 규칙이 있기는 하지만, 많은 예외 사항과 시간에 따른 언어의 변화, , 중의성과 모호성 문제 등을 전부 명세하기란 어렵다.
기계가 자연어를 표현하도록 규칙으로 명세하기가 어려운 상황에서 대안은 규칙 기반 접근이 아닌 기계가 주어진 자연어 데이터를 학습하게 하는 것이다.

과거에는 기계가 자연어를 학습하게 하는 방법으로 통계적인 접근을 사용했으나, 최근에는 인공 신경망을 사용하는 방법이 자연어 처리에서 더 좋은 성능을 얻고 있다.
번역기, 음성 인식 등과 같이 자연어 생성(Natural Language Generation, NLG)의 기반으로 사용되는 언어 모델도 마찬가지다.
통계적 언어 모델(Statistical Language Model, SLM)에서 다양한 구조의 인공 신경망을 사용한 언어 모델들로 대체되기 시작했다.

---

### 8.1 기존 N-gram 언어 모델의 한계

언어 모델은 문장에 확률을 할당하는 모델이며, 주어진 문맥으로부터 아직 모르는 단어를 예측하는 것을 언어 모델링이라고 한다고 한다.
다음은 이전 단어들로부터 다음 단어를 예측하는 언어 모델링(Language Modeling)의 예를 보여준다.

```python
# 다음 단어 예측하기
An adorable little boy is spreaing ____
```

위 문장을 가지고 앞서 배운 n-gram 언어 모델이 언어 모델링을 하는 방법을 복습해보자.

![img](https://wikidocs.net/images/page/21692/n-gram.PNG)

n-gram 언어 모델은 언어 모델링에 바로 앞의 n-1개의 단어만 참고한다.
4-gram 언어 모델이라고 가정해보자.
모델은 바로 앞 3개의 단어만 참고하며 더 앞의 단어들은 무시한다.
위의 예제에서 다음 단어 예측에 사용되는 단어는 boy, is, spreading이다.
$$
P(w\text{|boy is spreading}) = \frac{\text{count(boy is spreading}\ w)}{\text{count(boy is spreading)}}
$$
그 후에는 훈련 코퍼스에서 (n-1)-gram을 카운트한 것을 분모로, n-gram을 카운트한 것을 분자로 하여 다음 단어가 등장 확률을 예측했다.
예를 들어 갖고있는 코퍼스레서 boy is spreading가 1,000번, boy is spreading insults가 500번, boy is spreading smiles가 200번 등장했다면 각 확률은 아래와 같다.
$$
P(\text{insults|boy is spreading}) = 0.500
$$

$$
P(\text{smiles|boy is spreading}) = 0.200
$$

하지만 이러한 n-gram 언어 모델은 충분한 데이터를 관측하지 못하면 언어를 정확히 모델링하지 못하는 희소 문제(sparsity problem)가 있었다.
예를 들어 훈련 코퍼스에 'boy is spreading smile'라는 단어 시퀀스가 존재하지 않으면 n-gram 언어 모델에서 해당 단어 시퀀스의 확률 P(smiles|boy is spreading)는 0이 되버립니다.
이는 언어 모델이 예측하기에 boy is spreading 다음에는 smiles이란 단어가 나올 수 없다는 의미이지만 해당 단어 시퀀스는 현실에서 실제로는 많이 사용되므로 제대로된 모델릴이 아니다.

---

### 8.2 단어의 의미적 유사성

희소 문제는 기계가 단어 간 유사도를 알 수 있다면 해결할 수 있는 문제이다.
실제 사례를 이야기해보자.
최근 '톺아보다'라는 생소한 단어를 배웠고, '톺아보다'가 '샅샅이 살펴보다'와 유사한 의미임을 학습했다.
그리고 '발표 자료를 살펴보다'라는 표현 대신 '발표 자료를 톺아보다'라는 표현을 써봤다.
'발표 자료를 톺아보다'라는 예문을 어디서 읽은 적은 없지만 두 단어가 유사함을 학습하였으므로 단어를 대신 선택하여 자연어 생성을 할 수 있었다.

기계도 마찬가지이다.
'보도 자료를 살펴보다'라는 단어 시퀀스는 존재하지만, '발표 자료를 톺아보다'라는 단어 시퀀스는 존재하지 않는 코퍼스를 학습한 언어 모델이 있다고 가정해보자.
언어 모델은 아래 선택지에서 다음 단어를 예측해야 한다.
$$
P(\text{톺아보다|보도 자료를})
$$

$$
P(\text{냠냠하다|보도 자료를})
$$

'살펴보다'와 '톺아보다'의 유사성을 학습하였고 이를 근거로 두 선택지 중에서 '톺아보다'가 더 맞는 선택이라고 판단할 수 있다.
하지만 n-gram 언어 모델은 '보도 자료를' 다음에 '톺아보다'가 나올 확률 P(톺아보다|보도 자료를)P(톺아보다|보도 자료를)를 0으로 연산한다.
n-gram 언어 모델은 '살펴보다'와 '톺아보다'의 단어의 유사도를 학습한 적이 없으며, 예측에 고려할 수 없다.

만약이 언어 모델 또한 단어의 유사도를 학습할 수 있도록 설계한다면, 훈련 코퍼스에 없는 단어 시퀀스에 대한 예측이라도 유사한 단어가 사용된 단어 시퀀스를 참고하여 보다 정확한 예측을 할 수 있을 것이다.
그리고 이런 아이디어를 가지고 탄생한 언어 모델이 신경망 언어 모델 NNLM이다.
그리고 이 아이디어는 단어 간 유사도를 반영한 벡터를 만드는 워드 임베딩(word embedding)의 아이디어이기도 하다.
이제 NNLM이 어떻게 훈련 과정에서 단어의 유사도를 학습할 수 있는지 알아보자.

---

### 8.3 피드 포워드 신경망 언어 모델(NNLM)

NNLM이 언어 모델링을 학습하는 과정을 보자.

- 예문: "what will the fat cat sit on"

예를 들어 훈련 코퍼스에 위와 같은 문장이 있다고 해보자.
언어 모델은 주어진 단어 시퀀스로부터 다음 단어를 예측하는 모델이다.
훈련 과정에서는 'what will the fat cat'이라는 단어 시퀀스가 입력으로 주어지면, 다음 단어 'sit'을 예측하는 방식으로 훈련된다.

훈련 코퍼스가 준비된 상태에서 가장 먼저 해야 할 일은 기계가 단어를 인식할 수 있도록 모든 단어를 숫자로 인코딩 하는 것이다.
훈련 코퍼스에 7개의 단어만 존재한다고 가정했을 때 위 단어들에 대해서 다음과 같이 원-핫 인코딩을 할 수 있다.

```python
what = [1, 0, 0, 0, 0, 0, 0]
will = [0, 1, 0, 0, 0, 0, 0]
the = [0, 0, 1, 0, 0, 0, 0]
fat = [0, 0, 0, 1, 0, 0, 0]
cat = [0, 0, 0, 0, 1, 0, 0]
sit = [0, 0, 0, 0, 0, 1, 0]
on = [0, 0, 0, 0, 0, 0, 1]
```

모든 단어가 단어 집합(vocabulary)의 크기인 7의 차원을 가지는 원-핫 벡터가 되었다.
이제 이 원-핫 벡터들이 훈련을 위한 NNLM의 입력이면서 예측을 위한 레이블이기도 하다.
'what will the fat cat'를 입력을 받아서 'sit'을 예측하는 일은 기계에게 실제로는 what, will, the, fat, cat의 원-핫 벡터를 입력받아 sit의 원-핫 벡터를 예측하는 문제가 된다.

NNLM은 n-gram 언어 모델과 유사하게 다음 단어를 예측할 때, 앞의 모든 단어를 참고하는 것이 아니라 정해진 n개의 단어만을 참고한다.
예를 들어 n을 4라고 해보자.
이때, 언어 모델은 'what will the fat cat'라는 단어 시퀀스가 주어졌을 때, 다음 단어를 예측하기 위해 앞의 4개 단어 'will the fat cat'까지만 참고하고 그 앞 단어인 what은 무시한다.
이 범위를 윈도우(window)라고 하기도 하는데. 여기서 윈도우의 크기 n은 4이다.

![img](https://wikidocs.net/images/page/45609/nnlm1.PNG)

NNLM의 구조를 보자.
NNLM은 위의 그림과 같이 총 4개의 층(layer)으로 이루어진 인공 신경망이다.
우선 입력층(input layer)을 보자.
앞에서 윈도우의 크기는 4로 정하였으므로 입력은 4개의 단어 'what, will, the, fat'의 원-핫 벡터이다.
이제 출력층(Output layer)를 보자.
모델이 예측해야하는 정답에 해당되는 단어 sit의 원-핫 벡터는 출력층에서 모델이 예측한 값의 오차를 구하기 위해 사용될 예정이다.
그리고 이 오차로부터 손실 함수를 사용해 인공 신경망이 학습을 하게 된다.
이제 내부 메커니즘을 따라가보자.

4개의 원-핫 벡터를 입력 받은 NNLM은 다음층인 투사층(projection layer)을 지나게된다.
인공 신경망에서 입력층과 출력층 사이의 층은 보통 은닉층이라고 부르는데, 여기서 투사층이 일반 은닉층과 구별되는 특징은 가중치 행렬과의 연산은 이루어지지만 활성화 함수가 존재하지 않는다는 것이다.

투사층의 크기를 M으로 설정하면, 각 입력 단어들은 투사층에서 V × M 크기의 가중치 행렬과 곱해진다.
여기서 V는 단어 집합의 크기를 의미한다.
만약 원-핫 벡터의 차원이 7이고, M이 5라면 가중치 행렬 W는 7 × 5 행렬이 된다.

![img](https://wikidocs.net/images/page/45609/nnlm2_renew.PNG)

각 단어의 원-핫 벡터와 가중치 W 행렬의 곱이 어떻게 이루어지는지 보자.
위 그림에서는 각 원-핫 벡터를 x로 표기했다.
원-핫 벡터의 특성으로 인해 i번째 인덱스에 1이라는 값을 가지고 그 외의 0의 값을 가지는 원-핫 벡터와 가중치 W 행렬의 곱은 사실 W행렬의 i번째 행을 그대로 읽어오는 것과(lookup) 동일하다.
그래서 이 작업을 룩업 테이블(lookup table)이라고 부른다.

이 룩업 테이블 작업을 거치면 V의 차원을 가지는 원-핫 벡터는 이보다 더 차원이 작은 M차원의 단어 벡터로 맵핑된다.
위 그림에서는 단어 fat을 의미하는 원-핫 벡터를 xfat으로 표현했고, 테이블 룩업 과정을 거친 후의 단어 벡터는 efat으로 표현했다.

이 벡터들은 초기에는 랜덤한 값을 가지지만 학습 과정에서 값이 계속 변경되는데 이 단어 벡터를 임베딩 벡터(embedding vector)라고 한다.

![img](https://wikidocs.net/images/page/45609/nnlm3_renew.PNG)

각 단어가 테이블 룩업을 통해 임베딩 벡터로 변경되고, 투사층에서 모든 임베딩 벡터들의 값은 연결(concatenation)이 된다.
x를 각 단어의 원-핫 벡터, NNLM이 예측하고자 하는 단어가 문장에서 t번째 단어라고 하고, 윈도우의 크기를 n, 룩업 테이블을 의미하는 함수를 lookup, 세미콜론(;)을 연결 기호로 하였을 때 투사층을 식으로 표현하면 아래와 같다.

- 투사층: 
  $$
  p^{layer} = (lookup(x_{t-n}); ...; lookup(x_{t-2}); lookup(x_{t-1})) = (e_{t-n}; ...; e_{t-2}; e_{t-1})
  $$

일반적인 은닉층이 활성화 함수를 사용하는 비선형층(nonlinear layer)인 것과는 달리 투사층은 활성화 함수가 존재하지 않는 선형층(linear layer)이라는 점이 다소 생소하지만, 이 다음부터는 다시 은닉층을 사용하므로 일반적인 피드 포워드 신경망과 동일하다.

![img](https://wikidocs.net/images/page/45609/nnlm4.PNG)

투사층의 결과는 h의 크기를 가지는 은닉층을 지난다.
일반적인 피드 포워드 신경망에서 은닉층을 지난다는 것은 은닉층의 입력은 가중치 곱해진 후 편향이 더해져 활성화 함수의 입력이 된다는 의미이다.
이때의 가중치와 편향을 Wh와 bh이라고 하고, 은닉층의 활성화 함수를 하이퍼볼릭탄젠트 함수라고 하였을 때, 은닉층을 식으로 표현하면 아래와 같다.

- 은닉층: 
  $$
  h^{layer} = tanh(W_{h}p^{layer} + b_{h})
  $$

![img](https://wikidocs.net/images/page/45609/nnlm5_final.PNG)

은닉층의 출력은 이제 V의 크기를 가지는 출력층으로 향한다.
이 과정에서 다시 또 다른 가중치와 곱해지고 편향이 더해지면, 입력이었던 원-핫 벡터들과 동일하게 V차원의 벡터를 얻는다.
만약 입력 벡터의 차원이 7이었다면 여기서 나오는 벡터도 마찬가지다.

출력층에서는 활성화 함수로 소프트맥스(softmax) 함수를 사용하는데, V차원의 벡터는 소프트맥스 함수를 지나면서 각 원소는 0과 1사이의 실수값을 가지며 총 합은 1이 되는 상태로 바뀐다.
이렇게 나온 벡터를 NNLM의 예측값이라는 의미에서 y^라고 합시다.
이를 식으로 표현하면 아래와 같다.

- 출력층: 
  $$
  \hat{y} = softmax(W_{y}h^{layer} + b_{y})
  $$

벡터 y^의 각 차원 안에서의 값이 의미하는 것은 이와 같다.
y^의 j번째 인덱스가 가진 0과 1사이의 값은 j번째 단어가 다음 단어일 확률을 나타낸다.
그리고 y^는 실제값. 즉, 실제 정답에 해당되는 단어인 원-핫 벡터의 값에 가까워져야 한다.
실제값에 해당되는 다음 단어를 y라고 했을 때, 이 두 벡터가 가까워지게 하기위해서 NNLM는 손실 함수로 cross-entropy 함수를 사용한다.
그리고 역전파가 이루어지면서 가중치 행렬들이 학습되는데, 이 과정에서 임베딩 벡터값들도 학습이 된다.

이번 예제에서는 7개의 단어만 사용했지만, 만약 충분한 훈련 데이터가 있다는 가정 하에 NNLM이 얻을 수 있는 이점은 무엇일까.
NNLM의 핵심은 충분한 양의 훈련 코퍼스를 위와 같은 과정으로 학습한다면 결과적으로 수많은 문장에서 유사한 목적으로 사용되는 단어들은 결국 유사한 임베딩 벡터값을 얻게되는 것에 있다.
이렇게 되면 훈련이 끝난 후 다음 단어를 예측 과정에서 훈련 코퍼스에서 없던 단어 시퀀스라고 하더라도 다음 단어를 선택할 수 있다.

그리고 단어 간 유사도를 구할 수 있는 임베딩 벡터의 아이디어는 Word2Vec, FastText, GloVe 등으로 발전되어서 딥 러닝 모델에서는 필수적으로 사용되는 방법이 되었다.

---

### 8.4 NNLM의 이점과 한계

NNLM은 기존 n-gram 언어 모델의 한계를 몇 가지 개선하였지만 또한 여전히 가지는 문제점이 있다.

1. 기존 모델에서의 개선점
   NNLM은 단어의 유사도를 단어를 표현하기 위해 밀집 벡터(dense vector)를 사용하므로서 단어의 유사도를 표현할 수 있었다.
   그리고 이를 통해 희소 문제(sparsity problem)를 해결하였다.
   밀집 벡터란 벡터의 원소들이 실수값을 가지면서, 원-핫 벡터보다 저차원을 가지는 벡터를 말한다.
   반면, 원-핫 벡터는 벡터의 원소값이 대부분이 0이란 의미에서 희소 벡터(sparse vector)라고 부른다.

   또한 더 이상 모든 n-gram을 저장하지 않아도 된다는 점에서 n-gram 언어 모델보다 저장 공간의 이점을 가진다.

2. 고정된 길이의 입력(Fixed-length input)
   NNLM이 극복하지 못한 한계 또한 존재한다.
   NNLM은 n-gram 언어 모델과 마찬가지로 다음 단어를 예측하기 위해 모든 이전 단어를 참고하는 것이 아니라, 정해진 n개의 단어만을 참고한다.
   이는 버려지는 단어들이 가진 문맥 정보는 참고할 수 없음을 의미한다.

   훈련 코퍼스에 있는 각 문장의 길이는 전부 다를 수 있으므로, 이를 개선하기 위해서는 모델이 매번 다른 길이의 입력 시퀀스에 대해서 도 처리할 수 있는 능력이 필요하다.
   피드 포워드 신경망으로 만든 언어 모델이 이를 할 수 없다면, 피드 포워드 신경망이 아닌 다른 신경망을 사용하면 된다.

